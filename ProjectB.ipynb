{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Title\n",
    "### Group 14 \n",
    "\n",
    "> Github repository: https://github.com/LivDreyer/CSS24.git\n",
    "\n",
    "> Shortlog of git commits:\n",
    "- x  LivDreyer\n",
    "- x  AIAndreas\n",
    "- x  FelixxAI\n",
    "\n",
    "\n",
    "> Contribution: The workload was distributed equally between all members of the group. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Crowd](crowd.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import networkx as nx\n",
    "import netwulf as nu\n",
    "from networkx.readwrite import json_graph\n",
    "import random\n",
    "import pandas as pd\n",
    "import ast\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "import re\n",
    "import requests\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import spotipy\n",
    "from spotipy.oauth2 import SpotifyClientCredentials\n",
    "from bs4 import BeautifulSoup\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "* [1. Motivation](#motivation)\n",
    "    * [1.1 Datasets and motivation](#datasets)\n",
    "    * [1.2 Goal for the end user's experience](#goaluser)\n",
    "* [2. Data construction and Basic Statistics](#datacons)\n",
    "    * [2.1 Data collection from the Spotify API](#spotify)\n",
    "    * [2.2 Data collection from the Genius API](#textdata)\n",
    "        * [2.2.1 Creating data for textual analysis](#textdata_create)\n",
    "        * [2.2.2 Basic Statistics of textual data](#basic_stats_text)\n",
    "    * [2.3 Artist collaboration network data](#networkdata)\n",
    "        * [2.3.1 Preprocessing and cleaning](#networkpreclean)\n",
    "        * [2.3.2 Creation of network](#creation_of_network)\n",
    "        * [2.3.3 Basic statistics - network data](#basic_stats_network)\n",
    "* [3. Data analysis](#data_analysis)\n",
    "    * [3.1 Network analysis](#network_analysis)\n",
    "        * [3.1.1 Degree distribution](#degree_distribution)\n",
    "        * [3.1.2 Assortivity](#assortivity)\n",
    "        * [3.1.3 Communities](#communities)\n",
    "    * [3.2 Textual analysis](#text_analysis)\n",
    "        * [3.2.1 TF-IDF](#TF_IDF)\n",
    "        * [3.2.2 Wordclouds](#wordclouds)\n",
    "* [4. Discussion](#discussion)\n",
    "* [5. References](#references)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Motivation <a class=\"anchor\" id=\"motivation\"></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 What are our datasets and what is our motivation for choosing these? <a class=\"anchor\" id=\"datasets\"></a>\n",
    "\n",
    "This project aims to answer the research question: \"What is the relationship between artist collaboration patterns, popularity, and lyrical expression of genre themes?\". To answer this question, we decided to center our network analysis around the worlds largest music streaming service: Spotify. Given our project's focus on artist popularity, using a platform such as Spotify for insights is logical. With streaming services contributing to 84% of the music industry's revenue, and Spotify holding a dominant market share of 30.5% [1], it offers a comprehensive insight to artist popularity. This project utilizes the [Spotify API](https://developer.spotify.com/documentation/web-api) to gain insight into artist collaborations and the popularity of artists.\n",
    "\n",
    "For the textual analysis [Genius](https://genius.com), among others, serve as an \"online music encyclopidia\" [2]. Although the textual analysis in this project could utilize a variety of song lyric API's, [Genius' API](https://docs.genius.com) offers the ability for artists and users to annotate lyrics, making it an interesting choice for analyses in the future. \n",
    "\n",
    "This projects takes starting point in the Kaggle dataset [\"US Top 10K Artists and Their Popular Songs\"](https://www.kaggle.com/datasets/spoorthiuk/us-top-10k-artists-and-their-popular-songs). The dataset, created by Spoorthi Uday Karakaraddi, was collected using the Spotify API and features several attributes of the top 10k artists in the US in 2023. It serves as the foundation for constructing the dataset used for network analysis, which is subsequently used for constructing the dataset for our textual analysis. \n",
    "\n",
    "[1] https://explodingtopics.com/blog/music-streaming-stats\n",
    "\n",
    "[2] https://en.wikipedia.org/wiki/Genius_(company)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Our goal for the end user's experience <a class=\"anchor\" id=\"goaluser\"></a>\n",
    "\n",
    "Text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data construction and basic statistics <a class=\"anchor\" id=\"datacons\"></a>\n",
    "\n",
    "Since this project is working with API's, we are constructing the datasets from API data taking our starting point, as mentioned, in the Kaggle dataset [\"US Top 10K Artists and Their Popular Songs\"](https://www.kaggle.com/datasets/spoorthiuk/us-top-10k-artists-and-their-popular-songs). To provide a clear explanation of our course of action, this data section will be divided into the construction of the dataset used for network analysis and subsequently the one used for the text analysis after the initial work of fetching data from the Spotify API. Each block of code will be labeled with the name of the resulting csv-file. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> **Note:** In this project, a collaboration between two artists is considered one if an artist have a featuring artist on their songs. An example could be the artist \"Rihanna\". On her song titled \"Consideration\", we see the artist \"SZA\" is featured, which in this project will be considered a collaboration. To explore artists collaboration patterns, we used the \"US Top 10K Artists and Their Popular Songs\"-dataset to create our first list of artists.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Data collection from the Spotify API <a class=\"anchor\" id=\"spotify\"></a>\n",
    "\n",
    "The first query from the Spotify API consisted of retrieving the top 10 tracks of each artist to reveal possible featuring artists. Due to a rate limit of 5000 requests per day on queries from the Spotify API, with the API key being valid for only one hour, only the top 4250 artists, ranked by Spotify's measure of popularity, from the Kaggle dataset was used. This resulted in a dataframe of the song ID, song Name, main artist, featured artist(s) of the song, and the genre of the song. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Complete_Songs_with_Artists_and_Features.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Spotify API credentials\n",
    "client_id = '#'  \n",
    "client_secret = '#'  \n",
    "\n",
    "def refresh_token():\n",
    "    \"\"\" Refresh the Spotify API token. \"\"\"\n",
    "    url = 'https://accounts.spotify.com/api/token'\n",
    "    payload = {'grant_type': 'client_credentials'}\n",
    "    response = requests.post(url, auth=(client_id, client_secret), data=payload)\n",
    "    if response.status_code == 200:\n",
    "        new_token = response.json()['access_token']\n",
    "        logging.info(\"Token refreshed successfully.\")\n",
    "        return new_token\n",
    "    else:\n",
    "        logging.error(f\"Failed to refresh token: {response.text}\")\n",
    "        raise Exception(\"Failed to refresh token\")\n",
    "\n",
    "# Initial token\n",
    "token = refresh_token()\n",
    "headers = {'Authorization': f'Bearer {token}'}\n",
    "\n",
    "def get_top_tracks(artist_id, retry_count=0):\n",
    "    \"\"\" Fetch top tracks for a given artist ID from Spotify, handling rate limits dynamically. \"\"\"\n",
    "    global headers\n",
    "    top_tracks_url = f\"https://api.spotify.com/v1/artists/{artist_id}/top-tracks?country=US\"\n",
    "    response = requests.get(top_tracks_url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        return [(track['id'], track['name'], [artist['name'] for artist in track['artists']]) for track in data.get('tracks', [])]\n",
    "    elif response.status_code == 429:\n",
    "        retry_after = int(response.headers.get(\"Retry-After\", 30))\n",
    "        wait_time = min(retry_after, 30)  # Cap at 30 seconds to avoid overly long delays\n",
    "        logging.warning(f\"Rate limit exceeded, retrying after {wait_time} seconds...\")\n",
    "        time.sleep(wait_time)\n",
    "        return get_top_tracks(artist_id, retry_count + 1) if retry_count < 5 else []\n",
    "    elif response.status_code == 401 and retry_count < 5:\n",
    "        logging.warning(\"Token expired, refreshing token...\")\n",
    "        headers['Authorization'] = 'Bearer ' + refresh_token()\n",
    "        return get_top_tracks(artist_id, retry_count + 1)\n",
    "    else:\n",
    "        logging.error(f\"Failed to fetch data: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "def separate_artists(artists, main_artist_name):\n",
    "    \"\"\" Separate main artist from featured artists. \"\"\"\n",
    "    featured_artists = [artist for artist in artists if artist != main_artist_name]\n",
    "    return main_artist_name, ', '.join(featured_artists)\n",
    "\n",
    "def gather_top_tracks(df):\n",
    "    \"\"\" Process each artist and fetch their top tracks. \"\"\"\n",
    "    songs = []\n",
    "    for _, row in tqdm(df.iterrows(), total=df.shape[0], desc=\"Fetching top tracks\"):\n",
    "        artist_id = row['ID']\n",
    "        results = get_top_tracks(artist_id)\n",
    "        for song_id, song_name, artists in results:\n",
    "            main_artist, features = separate_artists(artists, row['Name'])\n",
    "            songs.append([song_id, song_name, main_artist, features])\n",
    "    return songs\n",
    "\n",
    "\n",
    "df_artists = pd.read_csv('Artists.csv')\n",
    "first_part = df_artists.iloc[:4250]\n",
    "logging.info(\"Processing the first 4250 artists...\")\n",
    "song_data = gather_top_tracks(first_part)\n",
    "songs_df = pd.DataFrame(song_data, columns=['Song ID', 'Song Name', 'Main Artist', 'Featured Artists', 'Genres'])\n",
    "\n",
    "# Save the data to CSV\n",
    "songs_df.to_csv('Complete_Songs_with_Artists_and_Features.csv', index=False)\n",
    "logging.info(\"Data saved to Complete_Songs_with_Artists_and_Features.csv.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then created a dataframe of the 4250 artists from the Kaggle dataset containing the following information: Main artist, Top 10 tracks, Artist ID, Genre (categorized by Spotify), Popularity Score, Follower Count, and their URI. See the following code block. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Final_Artist_Tracks_Info.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the datasets\n",
    "songs_df = pd.read_csv('Complete_Songs_with_Artists_and_Features.csv')\n",
    "artists_info_df = pd.read_csv('Artists.csv')\n",
    "\n",
    "songs_df['Main Artist'] = songs_df['Main Artist'].str.strip().str.lower()\n",
    "artists_info_df['Name'] = artists_info_df['Name'].str.strip().str.lower()\n",
    "top_tracks = songs_df.groupby('Main Artist')['Song Name'].apply(list).reset_index()\n",
    "top_tracks.rename(columns={'Main Artist': 'Name'}, inplace=True)\n",
    "final_df = pd.merge(top_tracks, artists_info_df, on='Name', how='left')\n",
    "final_df.rename(columns={'Name': 'Main Artist'}, inplace=True)\n",
    "final_df = final_df[['Main Artist', 'Song Name', 'ID', 'Genres', 'Popularity', 'Followers', 'URI']]\n",
    "\n",
    "# Save the final merged dataset to a new CSV file\n",
    "final_df.to_csv('Final_Artist_Tracks_Info.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now both have a dataframe of the first 4250 artists and the attributes previously mentioned and know which artists are featured on those 4250 artists top tracks. We will now query the same information for the featured artists, so we can merge the data, creating a final dataset.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Complete_Artists_Info.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spotipy\n",
    "from spotipy.oauth2 import SpotifyClientCredentials\n",
    "from tqdm import tqdm\n",
    "\n",
    "sp = spotipy.Spotify(auth_manager=SpotifyClientCredentials(client_id=\"#\", client_secret=\"#\"))\n",
    "\n",
    "# Function to search for an artist on Spotify and get their info\n",
    "def get_artist_info(artist_name):\n",
    "    try:\n",
    "        results = sp.search(q='artist:' + artist_name, type='artist', limit=1)\n",
    "        items = results['artists']['items']\n",
    "        if items:\n",
    "            artist = items[0]\n",
    "            return {\n",
    "                'Name': artist['name'],\n",
    "                'ID': artist['id'],\n",
    "                'Genres': ', '.join(artist['genres']),\n",
    "                'Popularity': artist['popularity'],\n",
    "                'Followers': artist['followers']['total'],\n",
    "                'URI': artist['uri']\n",
    "            }\n",
    "    except spotipy.client.SpotifyException as e:\n",
    "        print(f\"Spotify API error for {artist_name}: {e}\")\n",
    "    return None\n",
    "\n",
    "# Read the text file with artist names\n",
    "with open('Unique_Features.txt', 'r') as file:\n",
    "    unique_features = file.read().splitlines()\n",
    "\n",
    "artists_to_query = unique_features\n",
    "artists_info = []\n",
    "for artist_name in tqdm(artists_to_query, desc='Querying artists'):\n",
    "    artist_info = get_artist_info(artist_name)\n",
    "    if artist_info:\n",
    "        artists_info.append(artist_info)\n",
    "    else:\n",
    "        print(f\"No data found for artist: {artist_name}\")\n",
    "\n",
    "artists_df = pd.DataFrame(artists_info)\n",
    "artists_df.to_csv('Complete_Artists_Info.csv', index=False)\n",
    "print(\"Finished collecting all artists' information.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now found the following information on the featured artists: Artist, Top 10 tracks, Artist ID, Genre (categorized by Spotify), Popularity Score, Follower Count, and their URI. To obtain the top 10 tracks for each of the featuring artists, who are now just denoted as artists as well, we query the Spotify API yet again. As we are now interested in the top 10 tracks of 11260 artists, we carry out the following code in three sections. For every run of the code, we obtain about 4000 artists songs. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The following two blocks of code results in Feature_Artist_Songs_Combined.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Spotify API credentials\n",
    "client_id = '#'  \n",
    "client_secret = '#'  \n",
    "\n",
    "def refresh_token():\n",
    "    \"\"\" Refresh the Spotify API token. \"\"\"\n",
    "    url = 'https://accounts.spotify.com/api/token'\n",
    "    payload = {'grant_type': 'client_credentials'}\n",
    "    response = requests.post(url, auth=(client_id, client_secret), data=payload)\n",
    "    if response.status_code == 200:\n",
    "        new_token = response.json()['access_token']\n",
    "        logging.info(\"Token refreshed successfully.\")\n",
    "        return new_token\n",
    "    else:\n",
    "        logging.error(f\"Failed to refresh token: {response.text}\")\n",
    "        raise Exception(\"Failed to refresh token\")\n",
    "\n",
    "# Initial token\n",
    "token = refresh_token()\n",
    "headers = {'Authorization': f'Bearer {token}'}\n",
    "\n",
    "def get_top_tracks(artist_id, retry_count=0):\n",
    "    \"\"\"Fetch top tracks for a given artist ID from Spotify, handling rate limits dynamically.\"\"\"\n",
    "    global headers\n",
    "    top_tracks_url = f\"https://api.spotify.com/v1/artists/{artist_id}/top-tracks?country=US\"\n",
    "    response = requests.get(top_tracks_url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        return [(track['id'], track['name'], [artist['name'] for artist in track['artists']]) for track in data.get('tracks', [])]\n",
    "    elif response.status_code == 429:\n",
    "        retry_after = int(response.headers.get(\"Retry-After\", 30))\n",
    "        wait_time = min(retry_after, 30)  # Cap at 30 seconds to avoid overly long delays\n",
    "        logging.warning(f\"Rate limit exceeded, retrying after {wait_time} seconds...\")\n",
    "        time.sleep(wait_time)\n",
    "        return get_top_tracks(artist_id, retry_count + 1) if retry_count < 5 else []\n",
    "    elif response.status_code == 401 and retry_count < 5:\n",
    "        logging.warning(\"Token expired, refreshing token...\")\n",
    "        headers['Authorization'] = 'Bearer ' + refresh_token()\n",
    "        return get_top_tracks(artist_id, retry_count + 1)\n",
    "    else:\n",
    "        logging.error(f\"Failed to fetch data: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "def separate_artists(artists, main_artist_name):\n",
    "    \"\"\"Separate main artist from featured artists.\"\"\"\n",
    "    featured_artists = [artist for artist in artists if artist != main_artist_name]\n",
    "    return main_artist_name, ', '.join(featured_artists)\n",
    "\n",
    "def gather_top_tracks(df):\n",
    "    \"\"\"Process each artist and fetch their top tracks.\"\"\"\n",
    "    songs = []\n",
    "    for _, row in tqdm(df.iterrows(), total=df.shape[0], desc=\"Fetching top tracks\"):\n",
    "        artist_id = row['ID']\n",
    "        results = get_top_tracks(artist_id)\n",
    "        for song_id, song_name, artists in results:\n",
    "            main_artist, features = separate_artists(artists, row['Name'])\n",
    "            songs.append([song_id, song_name, main_artist, features])\n",
    "    return songs\n",
    "\n",
    "# Load the dataset\n",
    "df_artists = pd.read_csv('Complete_Artists_Info.csv')\n",
    "\n",
    "# Split the dataset into 3 parts\n",
    "#first_half = df_artists.iloc[:4300]\n",
    "#first_half = df_artists.iloc[4300:8600]\n",
    "first_half = df_artists.iloc[8600:]\n",
    "\n",
    "\n",
    "logging.info(\"Processing the first half of the dataset...\")\n",
    "first_half_data = gather_top_tracks(first_half)\n",
    "songs_df = pd.DataFrame(first_half_data, columns=['Song ID', 'Song Name', 'Main Artist', 'Featured Artists'])\n",
    "#songs_df.to_csv('Feature_Artist_Songs_Part1.csv', index=False)\n",
    "#songs_df.to_csv('Feature_Artist_Songs_Part2.csv', index=False)\n",
    "songs_df.to_csv('Feature_Artist_Songs_Part3.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They are then merged into one single dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load each part of the dataset\n",
    "df_part1 = pd.read_csv('Feature_Artist_Songs_Part1.csv')\n",
    "df_part2 = pd.read_csv('Feature_Artist_Songs_Part2.csv')\n",
    "df_part3 = pd.read_csv('Feature_Artist_Songs_Part3.csv')\n",
    "\n",
    "df_combined = pd.concat([df_part1, df_part2, df_part3], ignore_index=True)\n",
    "df_combined.to_csv('Feature_Artist_Songs_Combined.csv', index=False)\n",
    "\n",
    "print(\"Combined DataFrame Info:\")\n",
    "print(df_combined.info())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then create a full dataset of featured artists and their top 10 songs."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final_Feature_Artist_Tracks_Info.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the datasets\n",
    "songs_df = pd.read_csv('Feature_Artist_Songs_Combined.csv')\n",
    "artists_info_df = pd.read_csv('Featured/Complete_Feature_Artists_Info.csv')\n",
    "\n",
    "# Normalize artist names to ensure the case and trimming issues don't affect the merge\n",
    "songs_df['Main Artist'] = songs_df['Main Artist'].str.strip().str.lower()\n",
    "artists_info_df['Name'] = artists_info_df['Name'].str.strip().str.lower()\n",
    "\n",
    "# Aggregate the top tracks for each main artist\n",
    "top_tracks = songs_df.groupby('Main Artist')['Song Name'].apply(list).reset_index()\n",
    "top_tracks.rename(columns={'Main Artist': 'Name'}, inplace=True)\n",
    "final_df = pd.merge(top_tracks, artists_info_df, on='Name', how='left')\n",
    "final_df.rename(columns={'Name': 'Main Artist'}, inplace=True)\n",
    "final_df = final_df[['Main Artist', 'Song Name', 'ID', 'Genres', 'Popularity', 'Followers', 'URI']]\n",
    "final_df.to_csv('Final_Feature_Artist_Tracks_Info.csv', index=False)\n",
    "\n",
    "print(\"Final DataFrame head:\")\n",
    "print(final_df.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly we combine the *Final_Feature_Artist_Tracks_Info.csv* and *Final_Artist_Tracks_Info.csv* into one dataframe with the following attributes: Artist, Top songs, Artist ID, Genre, Popularity score, Followers, and URI. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Combined_Artist_Tracks_Info.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "featured_tracks = pd.read_csv('Featured\\\\Final_Feature_Artist_Tracks_Info.csv')\n",
    "final_artist_tracks = pd.read_csv('Final_Artist_Tracks_Info.csv')\n",
    "\n",
    "combined_data = pd.concat([featured_tracks, final_artist_tracks], ignore_index=True)\n",
    "combined_data = combined_data.drop_duplicates()\n",
    "combined_data.to_csv('Combined_Artist_Tracks_Info.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now fetched all data needed from the Spotify API. The following sections will be divided into the data used for the network analysis followed by the data used for the textual analysis. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Data collection from the Genius API <a class=\"anchor\" id=\"textdata\"></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 Creating data for textual analysis <a class=\"anchor\" id=\"textdata_create\"></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use *Combined_Artist_Track_Info.csv* that has about 14k rows and the following attributes: Artist, Songs, ID, GEnre, Popularity, Followers, URI. We then work to rename genres. We do it the same way as seen above, but a slight difference is removing \"others\". We then find the top 10 most popular genres based on total follower count. Then find the top 100 artists within each of the top 10 popular genres and randomly choose one of their top tracks as a representation of a song within the given genre.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import ast  \n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv('Combined_Artist_Tracks_Info.csv')\n",
    "\n",
    "# Function to convert string representations of lists into actual list objects\n",
    "def parse_song_names(song_names):\n",
    "    try:\n",
    "       \n",
    "        return ast.literal_eval(song_names)\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "\n",
    "data['Song Name'] = data['Song Name'].apply(parse_song_names)\n",
    "\n",
    "def rename_genres(original_genre_name):\n",
    "    pop_acronyms = ['pop']\n",
    "    rock_acronyms = ['rock', 'metal', 'punk', 'grunge']\n",
    "    hip_hop_acronyms = ['hip hop', 'rap', 'trap']\n",
    "    rnb_acronyms = ['r&b', 'jazz', 'blues', 'funk', 'lounge', 'soul']\n",
    "    country_acronyms = ['country']\n",
    "    indie_acronyms = ['indie']\n",
    "    electronic_acronyms = ['electronic', 'electro', 'edm', 'house', 'techno', 'dubstep', 'basshall', 'bass']\n",
    "    latin_acronyms = ['latino', 'corrido', 'latin', 'banda', 'ranchera', 'mariachi', 'cantautor', 'arrocha', 'sertanejo', 'vallenato']\n",
    "    raggae_acronyms = ['reggaeton', 'reggae']\n",
    "    hindi_acronyms = ['bollywood', 'filmi']\n",
    "    hollywood_acronyms = ['hollywood', 'soundtrack', 'movie tunes']\n",
    "    \n",
    "    new_genre_name = []\n",
    "    if type(original_genre_name) != str:\n",
    "        original_genre_name = str(original_genre_name)\n",
    "    if any(x in original_genre_name for x in pop_acronyms):\n",
    "        new_genre_name.append('pop')\n",
    "    if any(x in original_genre_name for x in rock_acronyms):\n",
    "        new_genre_name.append('rock')\n",
    "    if any(x in original_genre_name for x in hip_hop_acronyms):\n",
    "        new_genre_name.append('hip hop')\n",
    "    if any(x in original_genre_name for x in rnb_acronyms):\n",
    "        new_genre_name.append('r&b')\n",
    "    if any(x in original_genre_name for x in electronic_acronyms):\n",
    "        new_genre_name.append('electronic')\n",
    "    if any(x in original_genre_name for x in country_acronyms):\n",
    "        new_genre_name.append('country')\n",
    "    if any(x in original_genre_name for x in indie_acronyms):\n",
    "        new_genre_name.append('indie')\n",
    "    if any(x in original_genre_name for x in latin_acronyms):\n",
    "        new_genre_name.append('latin')\n",
    "    if any(x in original_genre_name for x in raggae_acronyms):\n",
    "        new_genre_name.append('reggae')\n",
    "    if any(x in original_genre_name for x in hindi_acronyms):\n",
    "        new_genre_name.append('hindi/bollywood')\n",
    "    if any(x in original_genre_name for x in hollywood_acronyms):\n",
    "        new_genre_name.append('hollywood')\n",
    "    if not new_genre_name:\n",
    "        new_genre_name.append('other')\n",
    "    return new_genre_name\n",
    "\n",
    "# Apply the rename_genres function\n",
    "data['Genres'] = data['Genres'].apply(lambda x: rename_genres(x))\n",
    "\n",
    "# Exploding the list of genres into separate rows\n",
    "data_exploded = data.explode('Genres')\n",
    "# Exclude 'other' genre\n",
    "data_filtered = data_exploded[data_exploded['Genres'] != 'other']\n",
    "\n",
    "# Calculate total followers for each genre and identify the top 10 genres\n",
    "genre_followers = data_filtered.groupby('Genres')['Followers'].sum().reset_index()\n",
    "top_genres = genre_followers.nlargest(10, 'Followers')['Genres']\n",
    "\n",
    "# Prepare a DataFrame to collect all top 100 artists across genres\n",
    "all_top_artists = pd.DataFrame()\n",
    "\n",
    "# Find top 100 artists in each of the top 10 genres\n",
    "for genre in top_genres:\n",
    "    filtered_data = data_filtered[data_filtered['Genres'] == genre]\n",
    "    top_artists = filtered_data.groupby(['Main Artist', 'Genres'])['Followers'].sum().reset_index()\n",
    "    top_artists_sorted = top_artists.nlargest(100, 'Followers')\n",
    "    \n",
    "    # Add a column for a random song from each artist's list of songs\n",
    "    top_artists_sorted['Random Song'] = top_artists_sorted['Main Artist'].map(\n",
    "        lambda artist: random.choice(data[data['Main Artist'] == artist]['Song Name'].iloc[0])\n",
    "        if data[data['Main Artist'] == artist]['Song Name'].iloc[0] else None\n",
    "    )\n",
    "\n",
    "    all_top_artists = pd.concat([all_top_artists, top_artists_sorted], ignore_index=True)\n",
    "\n",
    "# Save the consolidated list of top 100 artists from each genre with a random song to a single CSV file\n",
    "all_top_artists.to_csv('Top_100_Artists_Across_Top_Genres_With_Random_Song.csv', index=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above csv-file will serve as our foundation for querying the Genius API. When we give the Genius API a song title, we receive an URL for the song on Genius' website back. From there we can webscrape the lyrics of the song, saving those to our dataframe alongside the above mentioned attributes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "GENIUS_API_TOKEN = '#'  # Replace with your actual Genius API token\n",
    "\n",
    "def clean_song_title(song_title):\n",
    "    # Truncate song title at the first occurrence of '(' or '-'\n",
    "    return re.split(r'\\(|-', song_title)[0].strip()\n",
    "\n",
    "def request_song_url(artist_name, song_title):\n",
    "    clean_title = clean_song_title(song_title)\n",
    "    base_url = 'https://api.genius.com'\n",
    "    headers = {'Authorization': 'Bearer ' + GENIUS_API_TOKEN}\n",
    "    search_url = f\"{base_url}/search?q={artist_name} {clean_title}\"\n",
    "    response = requests.get(search_url, headers=headers)\n",
    "    json = response.json()\n",
    "\n",
    "    for hit in json['response']['hits']:\n",
    "        if artist_name.lower() in hit['result']['primary_artist']['name'].lower() and clean_title.lower() in hit['result']['title'].lower():\n",
    "            return hit['result']['url']\n",
    "    return None\n",
    "\n",
    "def scrape_song_lyrics(url):\n",
    "    if url is None:\n",
    "        return \"URL not found\"\n",
    "    page = requests.get(url)\n",
    "    html = BeautifulSoup(page.text, 'html.parser')\n",
    "    lyrics_div = html.find('div', class_='lyrics') or html.find('div', class_=re.compile(r'Lyrics__Container'))\n",
    "    if lyrics_div:\n",
    "        lyrics = lyrics_div.get_text(strip=True)\n",
    "        lyrics = re.sub(r'[\\(\\[].*?[\\)\\]]', '', lyrics)\n",
    "        lyrics = os.linesep.join([s for s in lyrics.splitlines() if s])\n",
    "    else:\n",
    "        lyrics = \"Lyrics not found\"\n",
    "    return lyrics\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('Top_100_Artists_Across_Top_Genres_With_Random_Song.csv')\n",
    "\n",
    "# Add a new column for the lyrics\n",
    "data['Lyrics'] = ''\n",
    "\n",
    "# Process each row to fetch lyrics\n",
    "for index, row in data.iterrows():\n",
    "    url = request_song_url(row['Main Artist'], row['Random Song'])\n",
    "    lyrics = scrape_song_lyrics(url)\n",
    "    data.at[index, 'Lyrics'] = lyrics  # Store the lyrics in the DataFrame\n",
    "\n",
    "# Save the updated DataFrame with lyrics to a new CSV file\n",
    "data.to_csv('Top_100_Artists_Across_Top_Genres_With_Random_Song_with_Lyrics.csv', index=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some instances, Genius is not able to find the URL or the lyrics of the randomly chosen song, meaning we will have to adjust for that. We do it by fetching another song by the artist, and repeat until we have a dataset where only 3% of the 1000 songs are missing lyrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import ast  \n",
    "\n",
    "# Load datasets\n",
    "lyrics_data = pd.read_csv('Top_100_Artists_Across_Top_Genres_With_Random_Song_with_Lyrics.csv')\n",
    "tracks_data = pd.read_csv('Combined_Artist_Tracks_Info.csv')\n",
    "\n",
    "# Convert string representation of lists in 'Song Name' to actual list objects\n",
    "tracks_data['Song Name'] = tracks_data['Song Name'].apply(lambda x: ast.literal_eval(x))\n",
    "\n",
    "# Function to get a new random song that is not the same as the current one\n",
    "def get_new_song(artist, current_song):\n",
    "    possible_songs = tracks_data[tracks_data['Main Artist'] == artist]['Song Name'].iloc[0]\n",
    "    if current_song in possible_songs:\n",
    "        possible_songs.remove(current_song)  # Avoid picking the same song\n",
    "    return random.choice(possible_songs) if possible_songs else None\n",
    "\n",
    "# Find rows with 'URL not found' and 'Lyrics not found' then replace the 'Random Song' with a new random song\n",
    "for index, row in lyrics_data.iterrows():\n",
    "    if row['Lyrics'] == 'URL not found' or row['Lyrics'] == 'Lyrics not found':\n",
    "        new_song = get_new_song(row['Main Artist'], row['Random Song'])\n",
    "        if new_song:\n",
    "            lyrics_data.at[index, 'Random Song'] = new_song\n",
    "            lyrics_data.at[index, 'Lyrics'] = 'New song selected, lyrics need re-fetching'  # Update lyrics status\n",
    "\n",
    "# Save the modified lyrics dataset\n",
    "lyrics_data.to_csv('Top_100_Artists_Across_Top_Genres_With_Random_Song_with_Lyrics.csv', index=False)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "GENIUS_API_TOKEN = '#'  \n",
    "\n",
    "def clean_song_title(song_title):\n",
    "    \"\"\" Truncate song title at the first occurrence of '(' or '-' \"\"\"\n",
    "    return re.split(r'\\(|-', song_title)[0].strip()\n",
    "\n",
    "def request_song_url(artist_name, song_title):\n",
    "    \"\"\" Request song URL from Genius API \"\"\"\n",
    "    clean_title = clean_song_title(song_title)\n",
    "    base_url = 'https://api.genius.com'\n",
    "    headers = {'Authorization': 'Bearer ' + GENIUS_API_TOKEN}\n",
    "    search_url = f\"{base_url}/search?q={artist_name} {clean_title}\"\n",
    "    response = requests.get(search_url, headers=headers)\n",
    "    json = response.json()\n",
    "\n",
    "    for hit in json['response']['hits']:\n",
    "        if artist_name.lower() in hit['result']['primary_artist']['name'].lower() and clean_title.lower() in hit['result']['title'].lower():\n",
    "            return hit['result']['url']\n",
    "    return None\n",
    "\n",
    "def scrape_song_lyrics(url):\n",
    "    \"\"\" Scrape song lyrics from Genius URL \"\"\"\n",
    "    if url is None:\n",
    "        return \"URL not found\"\n",
    "    page = requests.get(url)\n",
    "    html = BeautifulSoup(page.text, 'html.parser')\n",
    "    lyrics_div = html.find('div', class_='lyrics') or html.find('div', class_=re.compile(r'Lyrics__Container'))\n",
    "    if lyrics_div:\n",
    "        lyrics = lyrics_div.get_text(strip=True)\n",
    "        lyrics = re.sub(r'[\\(\\[].*?[\\)\\]]', '', lyrics)\n",
    "        lyrics = os.linesep.join([s for s in lyrics.splitlines() if s])\n",
    "    else:\n",
    "        lyrics = \"Lyrics not found\"\n",
    "    return lyrics\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('Top_100_Artists_Across_Top_Genres_With_Random_Song_with_Lyrics.csv')\n",
    "\n",
    "# Process each row to fetch lyrics only for entries marked as needing re-fetching\n",
    "for index, row in data.iterrows():\n",
    "    if row['Lyrics'] == \"New song selected, lyrics need re-fetching\":\n",
    "        url = request_song_url(row['Main Artist'], row['Random Song'])\n",
    "        lyrics = scrape_song_lyrics(url)\n",
    "        data.at[index, 'Lyrics'] = lyrics  # Update the lyrics in the DataFrame\n",
    "\n",
    "# Update the csv file with the new lyrics\n",
    "data.to_csv('Top_100_Artists_Across_Top_Genres_With_Random_Song_with_Lyrics.csv', index=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 Basic statistics of textual data <a class=\"anchor\" id=\"basic_stats_text\"></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Artist collaboration network data <a class=\"anchor\" id=\"networkdata\"></a>\n",
    "\n",
    "For the network analysis, we will use the following csv-files created en the previous section: \n",
    "\n",
    "- Final_Feature_Artist_Tracks_Info.csv\n",
    "- Feature_Artist_Songs_Combined.csv\n",
    "- Complete_Songs_with_Artists_and_Features.csv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.1 Preprocessing and cleaning <a class=\"anchor\" id=\"networkpreclean\"></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create the final dataset used for the network analysis, we first take *Final_Feature_Artist_Tracks_Info.csv* with the attributes Main Artist, Genre, Popularity Score, and Followers and join on \"Main Artist\" with *Feature_Artist_Songs_Combined.csv* that have the attributes Song Name, Main Artist and Featured Artist(s). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_artist = pd.read_csv('Artists.csv')\n",
    "\n",
    "df1 = pd.read_csv('Complete_Songs_with_Artists_and_Features.csv', usecols=['Song Name','Main Artist', 'Featured Artists'])\n",
    "\n",
    "merged_df = df1.merge(df_artist, left_on='Main Artist', right_on='Name', how='left')\n",
    "\n",
    "df1_with_data = merged_df.drop(columns=['Name', 'ID', 'URI', 'Age', 'Country', 'Gender'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then join *Complete_Songs_with_Artists_and_Features.csv* with the Kaggle \"US 10k Top Artists and Their Popular Songs\"-dataset on the name of the Main Artist. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('Feature_Artist_Songs_Combined.csv', usecols=['Song Name','Main Artist', 'Featured Artists'])\n",
    "\n",
    "df3 = pd.read_csv('Final_Feature_Artist_Tracks_info.csv', usecols=['Main Artist', 'Genres', 'Popularity', 'Followers'])\n",
    "\n",
    "df2['Main Artist'] = df2['Main Artist'].str.lower()\n",
    "\n",
    "merged_df = df2.merge(df3, on='Main Artist', how='inner')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly we combine the two dataframes, resulting in the final one shown below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_with_data['Main Artist'] = df1_with_data['Main Artist'].str.lower()\n",
    "merged_df = merged_df[~merged_df['Main Artist'].isin(df1_with_data['Main Artist'])]\n",
    "\n",
    "extended_df = pd.concat([df1_with_data, merged_df], ignore_index=True)\n",
    "\n",
    "cleaned_df = extended_df.drop_duplicates()\n",
    "cleaned_df.reset_index(drop=True, inplace=True)\n",
    "cleaned_df.to_csv('Final_Songs_with_Artists_and_Features.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show df \n",
    "cleaned_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As of November 2023, Spotify had 6000 different genres and artists often have more than one genre assigned to them.  \n",
    "\n",
    "We have created new groups of genres based on \"umbrella\"-genres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_acronyms = ['pop']\t\n",
    "rock_acronyms = ['rock', 'metal', 'punk', 'grunge']\n",
    "hip_hop_acronyms = ['hip hop', 'rap', 'trap']\n",
    "rnb_acronyms = ['r&b', 'jazz', 'blues', 'funk', 'lounge', 'soul']\n",
    "country_acronyms = ['country']\n",
    "indie_acronyms = ['indie']\n",
    "electronic_acronyms = ['electronic','electro', 'edm', 'house', 'techno', 'dubstep', 'basshall', 'bass']\n",
    "latin_acronyms = ['latino', 'corrido', 'latin', 'banda', 'ranchera', 'mariachi', 'cantautor', 'arrocha', 'sertanejo', 'vallenato'] # cantautor should maybe be added to rock and pop\n",
    "raggae_acronyms = ['reggaeton', 'reggae']\n",
    "hindi_acronyms = ['bollywood', 'filmi']\n",
    "hollywood_acronyms = ['hollywood', 'soundtrack', 'movie tunes']\n",
    "\n",
    "def rename_genres(original_genre_name):\n",
    "    new_genre_name = []\n",
    "    if type(original_genre_name) != str:\n",
    "        original_genre_name = str(original_genre_name)\n",
    "    if any(x in original_genre_name for x in pop_acronyms):\n",
    "        new_genre_name.append('pop')\n",
    "    if any(x in original_genre_name for x in rock_acronyms):\n",
    "        new_genre_name.append('rock')\n",
    "    if any(x in original_genre_name for x in hip_hop_acronyms):\n",
    "        new_genre_name.append('hip hop')\n",
    "    if any(x in original_genre_name for x in rnb_acronyms):\n",
    "        new_genre_name.append('r&b')\n",
    "    if any(x in original_genre_name for x in electronic_acronyms):\n",
    "        new_genre_name.append('electronic')\n",
    "    if any(x in original_genre_name for x in country_acronyms):\n",
    "        new_genre_name.append('country')\n",
    "    if any(x in original_genre_name for x in indie_acronyms):\n",
    "        new_genre_name.append('indie')\n",
    "    if any(x in original_genre_name for x in latin_acronyms):\n",
    "        new_genre_name.append('latin')\n",
    "    if any(x in original_genre_name for x in raggae_acronyms):\n",
    "        new_genre_name.append('raggae')\n",
    "    if any(x in original_genre_name for x in hindi_acronyms):\n",
    "        new_genre_name.append('hindi/bollywood')\n",
    "    if any(x in original_genre_name for x in hollywood_acronyms):\n",
    "        new_genre_name.append('hollywood')\n",
    "    if not new_genre_name:\n",
    "        new_genre_name.append('other')\n",
    "    return new_genre_name\n",
    "\n",
    "df = pd.read_csv('Final_Songs_with_Artists_and_Features.csv')\n",
    "\n",
    "df['Genres'] = df['Genres'].apply(rename_genres) # Rename genres\n",
    "\n",
    "print(df['Genres'].head)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2 Creation of network <a class=\"anchor\" id=\"creation_of_network\"></a>\n",
    "\n",
    "Text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.3 Basic statistics - network data <a class=\"anchor\" id=\"basic_stats_network\"></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Analysis <a class=\"anchor\" id=\"data_analysis\"></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Network Analysis <a class=\"anchor\" id=\"network_analysis\"></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1 Degree Distribution <a class=\"anchor\" id=\"degree_distribution\"></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2 Assortivity <a class=\"anchor\" id=\"assortivity\"></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.3 Communities <a class=\"anchor\" id=\"communities\"></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Textual Analysis <a class=\"anchor\" id=\"text_analysis\"></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1 TF-IDF <a class=\"anchor\" id=\"TF_IDF\"></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2 Wordclouds <a class=\"anchor\" id=\"wordclouds\"></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Discussion <a class=\"anchor\" id=\"discussion\"></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. References <a class=\"anchor\" id=\"references\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
