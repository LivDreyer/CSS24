{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Title\n",
    "### Group 14 \n",
    "\n",
    "> Github repository: https://github.com/LivDreyer/CSS24.git\n",
    "\n",
    "> Shortlog of git commits:\n",
    "- x  LivDreyer\n",
    "- x  AIAndreas\n",
    "- x  FelixxAI\n",
    "\n",
    "\n",
    "> Contribution: The workload was distributed equally between all members of the group. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Crowd](crowd.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import networkx as nx\n",
    "import netwulf as nu\n",
    "from networkx.readwrite import json_graph\n",
    "import random\n",
    "import pandas as pd\n",
    "import ast\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "import re\n",
    "import requests\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import spotipy\n",
    "from spotipy.oauth2 import SpotifyClientCredentials\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "* [1. Motivation](#motivation)\n",
    "    * [1.1 Datasets and motivation](#datasets)\n",
    "    * [1.2 Goal for the end user's experience](#goaluser)  \n",
    "* [2. Data construction and Basic Statistics](#datacons)\n",
    "    * [2.1 Data collection from the Spotify API](#spotify)\n",
    "    * [2.2 Data collection from the Genius API](#textdata)\n",
    "        * [2.2.1 Creating data for textual analysis](#textdata_create)\n",
    "        * [2.2.2 Basic Statistics of textual data](#basic_stats_text)\n",
    "    * [2.3 Artist collaboration network data](#networkdata)\n",
    "        * [2.3.1 Preprocessing and cleaning](#networkpreclean)\n",
    "        * [2.3.2 Creation of network](#creation_of_network)\n",
    "        * [2.3.3 Basic statistics - network data](#basic_stats_network) \n",
    "* [3. Data analysis](#data_analysis)\n",
    "    * [3.1 Network analysis](#network_analysis)\n",
    "        * [3.1.1 Degree distribution](#degree_distribution)\n",
    "        * [3.1.2 Assortivity](#assortivity)\n",
    "        * [3.1.3 Communities](#communities)\n",
    "    * [3.2 Textual analysis](#text_analysis)\n",
    "        * [3.2.1 Motivation](#motivation_text)\n",
    "        * [3.2.1 TF-IDF](#TF_IDF)\n",
    "        * [3.2.2 Wordclouds](#wordclouds)\n",
    "* [4. Discussion](#discussion)\n",
    "* [5. References](#references)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Motivation <a class=\"anchor\" id=\"motivation\"></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 What are our datasets and what is our motivation for choosing these? <a class=\"anchor\" id=\"datasets\"></a>\n",
    "\n",
    "Since the 1990's we have seen a 30% increase in collaborations between artists in the music industry [3], and according to the \"Billboard year-end hot singels of 2023\"[4], 40% of the top 10 songs \n",
    "\n",
    "This project aims to answer the research question: \"What is the relationship between artist collaboration patterns, popularity, and lyrical expression of genre themes?\". \n",
    "\n",
    "To answer this question, we decided to center our network analysis around the worlds largest music streaming service: Spotify. Given our project's focus on artist popularity, using a platform such as Spotify for insights is logical. With streaming services contributing to 84% of the music industry's revenue, and Spotify holding a dominant market share of 30.5% [1], it offers a comprehensive insight to artist popularity. This project utilizes the [Spotify API](https://developer.spotify.com/documentation/web-api) to gain insight into artist collaborations and the popularity of artists.\n",
    "\n",
    "For the textual analysis [Genius](https://genius.com), among others, serve as an \"online music encyclopidia\" [2]. Although the textual analysis in this project could utilize a variety of song lyric API's, [Genius' API](https://docs.genius.com) offers the ability for artists and users to annotate lyrics, making it an interesting choice for analyses in the future. \n",
    "\n",
    "This projects takes starting point in the Kaggle dataset [\"US Top 10K Artists and Their Popular Songs\"](https://www.kaggle.com/datasets/spoorthiuk/us-top-10k-artists-and-their-popular-songs). The dataset, created by Spoorthi Uday Karakaraddi, was collected using the Spotify API and features several attributes of the top 10k artists in the US in 2023. It serves as the foundation for constructing the datasets used for network analysis, and the textual analysis. \n",
    "\n",
    "[1] https://explodingtopics.com/blog/music-streaming-stats\n",
    "\n",
    "[2] https://en.wikipedia.org/wiki/Genius_(company)\n",
    "\n",
    "[3] https://www.economist.com/graphic-detail/2018/02/02/popular-music-is-more-collaborative-than-ever\n",
    "\n",
    "[4] https://en.wikipedia.org/wiki/Billboard_Year-End_Hot_100_singles_of_2023\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Our goal for the end user's experience <a class=\"anchor\" id=\"goaluser\"></a>\n",
    "\n",
    "Text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data construction and basic statistics <a class=\"anchor\" id=\"datacons\"></a>\n",
    "\n",
    "Since this project is working with API's, we are constructing the datasets from API data taking our starting point, as mentioned, in the Kaggle dataset [\"US Top 10K Artists and Their Popular Songs\"](https://www.kaggle.com/datasets/spoorthiuk/us-top-10k-artists-and-their-popular-songs). To provide a clear explanation of our course of action, this data section is divided into three parts: data collection from the Spotify API, data collection from the Genius API used for textual analysis, and lastly the preprocessing of data for the network and subqequent creation of it. For clarity, each block of code will be preceded with the name of the resulting csv-file. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> **Note:** In this project, a collaboration between two artists is considered one if an artist have a featuring artist on their songs. An example could be the artist \"Rihanna\". On her song titled \"Consideration\", we see the artist \"SZA\" is featured, which in this project will be considered a collaboration. To explore artists collaboration patterns, we used the \"US Top 10K Artists and Their Popular Songs\"-dataset to create our first list of artists.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Data collection from the Spotify API <a class=\"anchor\" id=\"spotify\"></a>\n",
    "\n",
    "### Data Description\n",
    "In this project, we utilize datasets acquired from the Spotify API, which includes detailed information about artists and songs. Below, we describe the attributes used in these datasets:\n",
    "\n",
    "#### Artist Attributes\n",
    "- **Followers**: Information about the number of Spotify users who follow the artist. This number indicates the artist's fanbase size and can be an indicator of their popularity.\n",
    "- **Genres**: Genres associated with the artist. These genres help classify the artist's music and can affect discovery on the platform.\n",
    "- **ID**: Unique identifier for the artist on Spotify. This ID is essential for querying specific data about the artist from the API.\n",
    "- **Name**: The artist's name as it appears on Spotify. This is the primary way users find and interact with artists on the platform.\n",
    "- **Popularity**: A score between 0 and 100 that measures the artist's popularity on Spotify. This score is calculated from the popularity of all the artist's tracks.\n",
    "- **URI**: Spotify URI that provides a direct link to access the artist on the platform.\n",
    "\n",
    "#### Song Attributes\n",
    "- **Song ID**: Unique identifier for the song on Spotify.\n",
    "- **Song Name**: The official title of the song as listed on Spotify.\n",
    "- **Artist**: The name of the primary artist associated with the song. This does not automatically include featured artists unless noted in the song's metadata.\n",
    "- **Featured Artists**: This attribute is created by parsing the \"feat\" or \"ft.\" notation in the song title. It identifies additional artists who contributed to the track but may not be listed as the primary artist. This attribute is manually extracted and added to the dataset to highlight these collaborations.\n",
    "\n",
    "The first query from the Spotify API consisted of retrieving the top 10 tracks of each artist from the Kaggle dataset to reveal possible featuring artists. Due to a rate limit of 5000 requests per day on queries from the Spotify API, with the API key being valid for only one hour, only the top 4250 artists, ranked by Spotify's measure of popularity, from the Kaggle dataset was used. This resulted in a dataframe of the Song ID, Song Name, Main Artist, Featured Artist(s) of the song, and the Genre of the Artist. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Complete_Songs_with_Artists_and_Features.csv <a class=\"anchor\" id=\"CompleteSongs\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Spotify API credentials\n",
    "client_id = '#'  \n",
    "client_secret = '#'  \n",
    "\n",
    "def refresh_token():\n",
    "    \"\"\" Refresh the Spotify API token. \"\"\"\n",
    "    url = 'https://accounts.spotify.com/api/token'\n",
    "    payload = {'grant_type': 'client_credentials'}\n",
    "    response = requests.post(url, auth=(client_id, client_secret), data=payload)\n",
    "    if response.status_code == 200:\n",
    "        new_token = response.json()['access_token']\n",
    "        logging.info(\"Token refreshed successfully.\")\n",
    "        return new_token\n",
    "    else:\n",
    "        logging.error(f\"Failed to refresh token: {response.text}\")\n",
    "        raise Exception(\"Failed to refresh token\")\n",
    "\n",
    "# Initial token\n",
    "token = refresh_token()\n",
    "headers = {'Authorization': f'Bearer {token}'}\n",
    "\n",
    "def get_top_tracks(artist_id, retry_count=0):\n",
    "    \"\"\" Fetch top tracks for a given artist ID from Spotify, handling rate limits dynamically. \"\"\"\n",
    "    global headers\n",
    "    top_tracks_url = f\"https://api.spotify.com/v1/artists/{artist_id}/top-tracks?country=US\"\n",
    "    response = requests.get(top_tracks_url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        return [(track['id'], track['name'], [artist['name'] for artist in track['artists']]) for track in data.get('tracks', [])]\n",
    "    elif response.status_code == 429:\n",
    "        retry_after = int(response.headers.get(\"Retry-After\", 30))\n",
    "        wait_time = min(retry_after, 30)  # Cap at 30 seconds to avoid overly long delays\n",
    "        logging.warning(f\"Rate limit exceeded, retrying after {wait_time} seconds...\")\n",
    "        time.sleep(wait_time)\n",
    "        return get_top_tracks(artist_id, retry_count + 1) if retry_count < 5 else []\n",
    "    elif response.status_code == 401 and retry_count < 5:\n",
    "        logging.warning(\"Token expired, refreshing token...\")\n",
    "        headers['Authorization'] = 'Bearer ' + refresh_token()\n",
    "        return get_top_tracks(artist_id, retry_count + 1)\n",
    "    else:\n",
    "        logging.error(f\"Failed to fetch data: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "def separate_artists(artists, main_artist_name):\n",
    "    \"\"\" Separate main artist from featured artists. \"\"\"\n",
    "    featured_artists = [artist for artist in artists if artist != main_artist_name]\n",
    "    return main_artist_name, ', '.join(featured_artists)\n",
    "\n",
    "def gather_top_tracks(df):\n",
    "    \"\"\" Process each artist and fetch their top tracks. \"\"\"\n",
    "    songs = []\n",
    "    for _, row in tqdm(df.iterrows(), total=df.shape[0], desc=\"Fetching top tracks\"):\n",
    "        artist_id = row['ID']\n",
    "        results = get_top_tracks(artist_id)\n",
    "        for song_id, song_name, artists in results:\n",
    "            main_artist, features = separate_artists(artists, row['Name'])\n",
    "            songs.append([song_id, song_name, main_artist, features])\n",
    "    return songs\n",
    "\n",
    "\n",
    "df_artists = pd.read_csv('Artists.csv')\n",
    "first_part = df_artists.iloc[:4250]\n",
    "logging.info(\"Processing the first 4250 artists...\")\n",
    "song_data = gather_top_tracks(first_part)\n",
    "songs_df = pd.DataFrame(song_data, columns=['Song ID', 'Song Name', 'Main Artist', 'Featured Artists', 'Genres'])\n",
    "\n",
    "# Save the data to CSV\n",
    "songs_df.to_csv('Complete_Songs_with_Artists_and_Features.csv', index=False)\n",
    "logging.info(\"Data saved to Complete_Songs_with_Artists_and_Features.csv.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then created a dataframe of the 4250 artists from the Kaggle dataset containing the following information: Main artist, Top 10 tracks, Artist ID, Genre (categorized by Spotify), Popularity Score, Follower Count, and their URI."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Final_Artist_Tracks_Info.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "songs_df = pd.read_csv('Complete_Songs_with_Artists_and_Features.csv')\n",
    "artists_info_df = pd.read_csv('Artists.csv')\n",
    "\n",
    "songs_df['Main Artist'] = songs_df['Main Artist'].str.strip().str.lower()\n",
    "artists_info_df['Name'] = artists_info_df['Name'].str.strip().str.lower()\n",
    "top_tracks = songs_df.groupby('Main Artist')['Song Name'].apply(list).reset_index()\n",
    "top_tracks.rename(columns={'Main Artist': 'Name'}, inplace=True)\n",
    "final_df = pd.merge(top_tracks, artists_info_df, on='Name', how='left')\n",
    "final_df.rename(columns={'Name': 'Main Artist'}, inplace=True)\n",
    "final_df = final_df[['Main Artist', 'Song Name', 'ID', 'Genres', 'Popularity', 'Followers', 'URI']]\n",
    "\n",
    "# Save the final merged dataset to a new CSV file\n",
    "final_df.to_csv('Final_Artist_Tracks_Info.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now both have a dataframe of the first 4250 artists and the attributes previously mentioned, and know which artists are featured on those 4250 artists top tracks. We will now query the same information for the featured artists, so we can merge the data, creating a final dataset.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Complete_Artists_Info.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = spotipy.Spotify(auth_manager=SpotifyClientCredentials(client_id=\"#\", client_secret=\"#\"))\n",
    "\n",
    "# Function to search for an artist on Spotify and get their info\n",
    "def get_artist_info(artist_name):\n",
    "    try:\n",
    "        results = sp.search(q='artist:' + artist_name, type='artist', limit=1)\n",
    "        items = results['artists']['items']\n",
    "        if items:\n",
    "            artist = items[0]\n",
    "            return {\n",
    "                'Name': artist['name'],\n",
    "                'ID': artist['id'],\n",
    "                'Genres': ', '.join(artist['genres']),\n",
    "                'Popularity': artist['popularity'],\n",
    "                'Followers': artist['followers']['total'],\n",
    "                'URI': artist['uri']\n",
    "            }\n",
    "    except spotipy.client.SpotifyException as e:\n",
    "        print(f\"Spotify API error for {artist_name}: {e}\")\n",
    "    return None\n",
    "\n",
    "# Read the text file with artist names\n",
    "with open('Unique_Features.txt', 'r') as file:\n",
    "    unique_features = file.read().splitlines()\n",
    "\n",
    "artists_to_query = unique_features\n",
    "artists_info = []\n",
    "for artist_name in tqdm(artists_to_query, desc='Querying artists'):\n",
    "    artist_info = get_artist_info(artist_name)\n",
    "    if artist_info:\n",
    "        artists_info.append(artist_info)\n",
    "    else:\n",
    "        print(f\"No data found for artist: {artist_name}\")\n",
    "\n",
    "artists_df = pd.DataFrame(artists_info)\n",
    "artists_df.to_csv('Complete_Artists_Info.csv', index=False)\n",
    "print(\"Finished collecting all artists' information.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now found the following information on the featured artists: Artist, Top 10 tracks, Artist ID, Genre (categorized by Spotify), Popularity Score, Follower Count, and their URI. To obtain the top 10 tracks for each of the featuring artists, who are now just denoted as artists as well, we query the Spotify API yet again. As we are now interested in the top 10 tracks of 11260 artists, we carry out the following code in three sections due to the limitations of the Spotify API. For every run of the code, we obtain about 4000 artists songs. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The following two blocks of code results in Feature_Artist_Songs_Combined.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Spotify API credentials\n",
    "client_id = '#'  \n",
    "client_secret = '#'  \n",
    "\n",
    "def refresh_token():\n",
    "    \"\"\" Refresh the Spotify API token. \"\"\"\n",
    "    url = 'https://accounts.spotify.com/api/token'\n",
    "    payload = {'grant_type': 'client_credentials'}\n",
    "    response = requests.post(url, auth=(client_id, client_secret), data=payload)\n",
    "    if response.status_code == 200:\n",
    "        new_token = response.json()['access_token']\n",
    "        logging.info(\"Token refreshed successfully.\")\n",
    "        return new_token\n",
    "    else:\n",
    "        logging.error(f\"Failed to refresh token: {response.text}\")\n",
    "        raise Exception(\"Failed to refresh token\")\n",
    "\n",
    "# Initial token\n",
    "token = refresh_token()\n",
    "headers = {'Authorization': f'Bearer {token}'}\n",
    "\n",
    "def get_top_tracks(artist_id, retry_count=0):\n",
    "    \"\"\"Fetch top tracks for a given artist ID from Spotify, handling rate limits dynamically.\"\"\"\n",
    "    global headers\n",
    "    top_tracks_url = f\"https://api.spotify.com/v1/artists/{artist_id}/top-tracks?country=US\"\n",
    "    response = requests.get(top_tracks_url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        return [(track['id'], track['name'], [artist['name'] for artist in track['artists']]) for track in data.get('tracks', [])]\n",
    "    elif response.status_code == 429:\n",
    "        retry_after = int(response.headers.get(\"Retry-After\", 30))\n",
    "        wait_time = min(retry_after, 30)  # Cap at 30 seconds to avoid overly long delays\n",
    "        logging.warning(f\"Rate limit exceeded, retrying after {wait_time} seconds...\")\n",
    "        time.sleep(wait_time)\n",
    "        return get_top_tracks(artist_id, retry_count + 1) if retry_count < 5 else []\n",
    "    elif response.status_code == 401 and retry_count < 5:\n",
    "        logging.warning(\"Token expired, refreshing token...\")\n",
    "        headers['Authorization'] = 'Bearer ' + refresh_token()\n",
    "        return get_top_tracks(artist_id, retry_count + 1)\n",
    "    else:\n",
    "        logging.error(f\"Failed to fetch data: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "def separate_artists(artists, main_artist_name):\n",
    "    \"\"\"Separate main artist from featured artists.\"\"\"\n",
    "    featured_artists = [artist for artist in artists if artist != main_artist_name]\n",
    "    return main_artist_name, ', '.join(featured_artists)\n",
    "\n",
    "def gather_top_tracks(df):\n",
    "    \"\"\"Process each artist and fetch their top tracks.\"\"\"\n",
    "    songs = []\n",
    "    for _, row in tqdm(df.iterrows(), total=df.shape[0], desc=\"Fetching top tracks\"):\n",
    "        artist_id = row['ID']\n",
    "        results = get_top_tracks(artist_id)\n",
    "        for song_id, song_name, artists in results:\n",
    "            main_artist, features = separate_artists(artists, row['Name'])\n",
    "            songs.append([song_id, song_name, main_artist, features])\n",
    "    return songs\n",
    "\n",
    "# Load the dataset\n",
    "df_artists = pd.read_csv('Complete_Artists_Info.csv')\n",
    "\n",
    "# Split the dataset into 3 parts\n",
    "#first_half = df_artists.iloc[:4300]\n",
    "#first_half = df_artists.iloc[4300:8600]\n",
    "first_half = df_artists.iloc[8600:]\n",
    "\n",
    "\n",
    "logging.info(\"Processing the first half of the dataset...\")\n",
    "first_half_data = gather_top_tracks(first_half)\n",
    "songs_df = pd.DataFrame(first_half_data, columns=['Song ID', 'Song Name', 'Main Artist', 'Featured Artists'])\n",
    "#songs_df.to_csv('Feature_Artist_Songs_Part1.csv', index=False)\n",
    "#songs_df.to_csv('Feature_Artist_Songs_Part2.csv', index=False)\n",
    "songs_df.to_csv('Feature_Artist_Songs_Part3.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They are then merged into one single dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load each part of the dataset\n",
    "df_part1 = pd.read_csv('Feature_Artist_Songs_Part1.csv')\n",
    "df_part2 = pd.read_csv('Feature_Artist_Songs_Part2.csv')\n",
    "df_part3 = pd.read_csv('Feature_Artist_Songs_Part3.csv')\n",
    "\n",
    "df_combined = pd.concat([df_part1, df_part2, df_part3], ignore_index=True)\n",
    "df_combined.to_csv('Feature_Artist_Songs_Combined.csv', index=False)\n",
    "\n",
    "print(\"Combined DataFrame Info:\")\n",
    "print(df_combined.info())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then create a full dataset of featured artists and their top 10 songs."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final_Feature_Artist_Tracks_Info.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "songs_df = pd.read_csv('Feature_Artist_Songs_Combined.csv')\n",
    "artists_info_df = pd.read_csv('Featured/Complete_Feature_Artists_Info.csv')\n",
    "\n",
    "# Normalize artist names to ensure the case and trimming issues don't affect the merge\n",
    "songs_df['Main Artist'] = songs_df['Main Artist'].str.strip().str.lower()\n",
    "artists_info_df['Name'] = artists_info_df['Name'].str.strip().str.lower()\n",
    "\n",
    "# Aggregate the top tracks for each main artist\n",
    "top_tracks = songs_df.groupby('Main Artist')['Song Name'].apply(list).reset_index()\n",
    "top_tracks.rename(columns={'Main Artist': 'Name'}, inplace=True)\n",
    "final_df = pd.merge(top_tracks, artists_info_df, on='Name', how='left')\n",
    "final_df.rename(columns={'Name': 'Main Artist'}, inplace=True)\n",
    "final_df = final_df[['Main Artist', 'Song Name', 'ID', 'Genres', 'Popularity', 'Followers', 'URI']]\n",
    "final_df.to_csv('Final_Feature_Artist_Tracks_Info.csv', index=False)\n",
    "\n",
    "print(\"Final DataFrame head:\")\n",
    "print(final_df.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly we combine the *Final_Feature_Artist_Tracks_Info.csv* and *Final_Artist_Tracks_Info.csv* into one dataframe with the following attributes: Artist, Top songs, Artist ID, Genre, Popularity score, Followers, and URI. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Combined_Artist_Tracks_Info.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featured_tracks = pd.read_csv('Featured\\\\Final_Feature_Artist_Tracks_Info.csv')\n",
    "final_artist_tracks = pd.read_csv('Final_Artist_Tracks_Info.csv')\n",
    "\n",
    "combined_data = pd.concat([featured_tracks, final_artist_tracks], ignore_index=True)\n",
    "combined_data = combined_data.drop_duplicates()\n",
    "combined_data.to_csv('Combined_Artist_Tracks_Info.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now fetched all data needed from the Spotify API."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Data Collection from the Genius API <a class=\"anchor\" id=\"textdata\"></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 Creating Dataset for Textual Analysis <a class=\"anchor\" id=\"textdata_create\"></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When collecting data from the Genius API, we utilize the CSV-file *Combined_Artist_Track_Info.csv* generated by processing information from the Spotify API. The file is used to query the Genius API for URLs corresponding to particular song titles. These URLs enable us to perform web scraping on the website linked to the URL to extract the song lyrics. *Combined_Artist_Track_Info.csv* consists of around 14000 rows, in this case artists, and contain the following attribute for each: Artist, Songs, ID, Genre, Popularity, Followers, URI. \n",
    "\n",
    "One obstacle regarding the Spotify data is the number of genres that spotify use to partition their artists into. As of November 2023, Spotify had 6000 different genres [1], and though not all represented in this data, we still consider the number of genres excessive. This problem is not exclusive to the textual analysis, and will be touched upon in the network analysis as well. \n",
    "Due to the massive amount of different genres, we start by grouping artists in broader genres defined below. \n",
    "\n",
    "We then find the 10 most popular genres defined by total follower count, followed by identifying the top 100 most popular artists within each of the 10 genres. Next we choose one random song from the top 100 artists top 10 tracks, resulting in 100 songs, by 100 different artists with each genre totalling 1000 songs. \n",
    "\n",
    "[1] https://klara-alexeeva.medium.com/spotify-features-6-000-music-genres-some-of-them-are-made-up-1e3707f6f022\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_genres(original_genre_name):\n",
    "    pop_acronyms = ['pop']\n",
    "    rock_acronyms = ['rock', 'metal', 'punk', 'grunge']\n",
    "    hip_hop_acronyms = ['hip hop', 'rap', 'trap']\n",
    "    rnb_acronyms = ['r&b', 'jazz', 'blues', 'funk', 'lounge', 'soul']\n",
    "    country_acronyms = ['country']\n",
    "    indie_acronyms = ['indie']\n",
    "    electronic_acronyms = ['electronic', 'electro', 'edm', 'house', 'techno', 'dubstep', 'basshall', 'bass']\n",
    "    latin_acronyms = ['latino', 'corrido', 'latin', 'banda', 'ranchera', 'mariachi', 'cantautor', 'arrocha', 'sertanejo', 'vallenato']\n",
    "    raggae_acronyms = ['reggaeton', 'reggae']\n",
    "    hindi_acronyms = ['bollywood', 'filmi']\n",
    "    hollywood_acronyms = ['hollywood', 'soundtrack', 'movie tunes']\n",
    "    \n",
    "    new_genre_name = []\n",
    "    if type(original_genre_name) != str:\n",
    "        original_genre_name = str(original_genre_name)\n",
    "    if any(x in original_genre_name for x in pop_acronyms):\n",
    "        new_genre_name.append('pop')\n",
    "    if any(x in original_genre_name for x in rock_acronyms):\n",
    "        new_genre_name.append('rock')\n",
    "    if any(x in original_genre_name for x in hip_hop_acronyms):\n",
    "        new_genre_name.append('hip hop')\n",
    "    if any(x in original_genre_name for x in rnb_acronyms):\n",
    "        new_genre_name.append('r&b')\n",
    "    if any(x in original_genre_name for x in electronic_acronyms):\n",
    "        new_genre_name.append('electronic')\n",
    "    if any(x in original_genre_name for x in country_acronyms):\n",
    "        new_genre_name.append('country')\n",
    "    if any(x in original_genre_name for x in indie_acronyms):\n",
    "        new_genre_name.append('indie')\n",
    "    if any(x in original_genre_name for x in latin_acronyms):\n",
    "        new_genre_name.append('latin')\n",
    "    if any(x in original_genre_name for x in raggae_acronyms):\n",
    "        new_genre_name.append('reggae')\n",
    "    if any(x in original_genre_name for x in hindi_acronyms):\n",
    "        new_genre_name.append('hindi/bollywood')\n",
    "    if any(x in original_genre_name for x in hollywood_acronyms):\n",
    "        new_genre_name.append('hollywood')\n",
    "    if not new_genre_name:\n",
    "        new_genre_name.append('other')\n",
    "    return new_genre_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Combined_Artist_Tracks_Info.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load dataset\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCombined_Artist_Tracks_Info.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Function to convert string representations of lists into actual list objects\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse_song_names\u001b[39m(song_names):\n",
      "File \u001b[1;32mc:\\Users\\andre\\miniconda3\\envs\\CSS\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Users\\andre\\miniconda3\\envs\\CSS\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\andre\\miniconda3\\envs\\CSS\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32mc:\\Users\\andre\\miniconda3\\envs\\CSS\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1881\u001b[0m     f,\n\u001b[0;32m   1882\u001b[0m     mode,\n\u001b[0;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1889\u001b[0m )\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\andre\\miniconda3\\envs\\CSS\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Combined_Artist_Tracks_Info.csv'"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "data = pd.read_csv('Combined_Artist_Tracks_Info.csv')\n",
    "\n",
    "# Function to convert string representations of lists into actual list objects\n",
    "def parse_song_names(song_names):\n",
    "    try:\n",
    "       \n",
    "        return ast.literal_eval(song_names)\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "\n",
    "data['Song Name'] = data['Song Name'].apply(parse_song_names)\n",
    "\n",
    "# Apply the rename_genres function\n",
    "data['Genres'] = data['Genres'].apply(lambda x: rename_genres(x))\n",
    "\n",
    "# Exploding the list of genres into separate rows\n",
    "data_exploded = data.explode('Genres')\n",
    "# Exclude 'other' genre\n",
    "data_filtered = data_exploded[data_exploded['Genres'] != 'other']\n",
    "\n",
    "# Calculate total followers for each genre and identify the top 10 genres\n",
    "genre_followers = data_filtered.groupby('Genres')['Followers'].sum().reset_index()\n",
    "top_genres = genre_followers.nlargest(10, 'Followers')['Genres']\n",
    "\n",
    "# Prepare a DataFrame to collect all top 100 artists across genres\n",
    "all_top_artists = pd.DataFrame()\n",
    "\n",
    "# Find top 100 artists in each of the top 10 genres\n",
    "for genre in top_genres:\n",
    "    filtered_data = data_filtered[data_filtered['Genres'] == genre]\n",
    "    top_artists = filtered_data.groupby(['Main Artist', 'Genres'])['Followers'].sum().reset_index()\n",
    "    top_artists_sorted = top_artists.nlargest(100, 'Followers')\n",
    "    \n",
    "    # Add a column for a random song from each artist's list of songs\n",
    "    top_artists_sorted['Random Song'] = top_artists_sorted['Main Artist'].map(\n",
    "        lambda artist: random.choice(data[data['Main Artist'] == artist]['Song Name'].iloc[0])\n",
    "        if data[data['Main Artist'] == artist]['Song Name'].iloc[0] else None\n",
    "    )\n",
    "\n",
    "    all_top_artists = pd.concat([all_top_artists, top_artists_sorted], ignore_index=True)\n",
    "\n",
    "# Save the consolidated list of top 100 artists from each genre with a random song to a single CSV file\n",
    "all_top_artists.to_csv('Top_100_Artists_Across_Top_Genres_With_Random_Song.csv', index=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above CSV-file will serve as our foundation for querying the Genius API. When we give the Genius API a song title, we receive an URL for the song on Genius' website back. From there we can webscrape the lyrics of the song, saving those to our dataframe alongside the above mentioned attributes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GENIUS_API_TOKEN = '#'  # Replace with your actual Genius API token\n",
    "\n",
    "def clean_song_title(song_title):\n",
    "    # Truncate song title at the first occurrence of '(' or '-'\n",
    "    return re.split(r'\\(|-', song_title)[0].strip()\n",
    "\n",
    "def request_song_url(artist_name, song_title):\n",
    "    clean_title = clean_song_title(song_title)\n",
    "    base_url = 'https://api.genius.com'\n",
    "    headers = {'Authorization': 'Bearer ' + GENIUS_API_TOKEN}\n",
    "    search_url = f\"{base_url}/search?q={artist_name} {clean_title}\"\n",
    "    response = requests.get(search_url, headers=headers)\n",
    "    json = response.json()\n",
    "\n",
    "    for hit in json['response']['hits']:\n",
    "        if artist_name.lower() in hit['result']['primary_artist']['name'].lower() and clean_title.lower() in hit['result']['title'].lower():\n",
    "            return hit['result']['url']\n",
    "    return None\n",
    "\n",
    "def scrape_song_lyrics(url):\n",
    "    if url is None:\n",
    "        return \"URL not found\"\n",
    "    page = requests.get(url)\n",
    "    html = BeautifulSoup(page.text, 'html.parser')\n",
    "    lyrics_div = html.find('div', class_='lyrics') or html.find('div', class_=re.compile(r'Lyrics__Container'))\n",
    "    if lyrics_div:\n",
    "        lyrics = lyrics_div.get_text(strip=True)\n",
    "        lyrics = re.sub(r'[\\(\\[].*?[\\)\\]]', '', lyrics)\n",
    "        lyrics = os.linesep.join([s for s in lyrics.splitlines() if s])\n",
    "    else:\n",
    "        lyrics = \"Lyrics not found\"\n",
    "    return lyrics\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('Top_100_Artists_Across_Top_Genres_With_Random_Song.csv')\n",
    "\n",
    "# Add a new column for the lyrics\n",
    "data['Lyrics'] = ''\n",
    "\n",
    "# Process each row to fetch lyrics\n",
    "for index, row in data.iterrows():\n",
    "    url = request_song_url(row['Main Artist'], row['Random Song'])\n",
    "    lyrics = scrape_song_lyrics(url)\n",
    "    data.at[index, 'Lyrics'] = lyrics  # Store the lyrics in the DataFrame\n",
    "\n",
    "# Save the updated DataFrame with lyrics to a new CSV file\n",
    "data.to_csv('Top_100_Artists_Across_Top_Genres_With_Random_Song_with_Lyrics.csv', index=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some instances, Genius is unable to find the URL or the lyrics of the randomly chosen song, meaning we will have to adjust for that. We do it by fetching another song by the artist, and repeat until we have a completed dataset where only 3% of the 1000 songs are missing lyrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "lyrics_data = pd.read_csv('Top_100_Artists_Across_Top_Genres_With_Random_Song_with_Lyrics.csv')\n",
    "tracks_data = pd.read_csv('Combined_Artist_Tracks_Info.csv')\n",
    "\n",
    "# Convert string representation of lists in 'Song Name' to actual list objects\n",
    "tracks_data['Song Name'] = tracks_data['Song Name'].apply(lambda x: ast.literal_eval(x))\n",
    "\n",
    "# Function to get a new random song that is not the same as the current one\n",
    "def get_new_song(artist, current_song):\n",
    "    possible_songs = tracks_data[tracks_data['Main Artist'] == artist]['Song Name'].iloc[0]\n",
    "    if current_song in possible_songs:\n",
    "        possible_songs.remove(current_song)  # Avoid picking the same song\n",
    "    return random.choice(possible_songs) if possible_songs else None\n",
    "\n",
    "# Find rows with 'URL not found' and 'Lyrics not found' then replace the 'Random Song' with a new random song\n",
    "for index, row in lyrics_data.iterrows():\n",
    "    if row['Lyrics'] == 'URL not found' or row['Lyrics'] == 'Lyrics not found':\n",
    "        new_song = get_new_song(row['Main Artist'], row['Random Song'])\n",
    "        if new_song:\n",
    "            lyrics_data.at[index, 'Random Song'] = new_song\n",
    "            lyrics_data.at[index, 'Lyrics'] = 'New song selected, lyrics need re-fetching'  # Update lyrics status\n",
    "\n",
    "# Save the modified lyrics dataset\n",
    "lyrics_data.to_csv('Top_100_Artists_Across_Top_Genres_With_Random_Song_with_Lyrics.csv', index=False)\n",
    "\n",
    "\n",
    "\n",
    "GENIUS_API_TOKEN = '#'  \n",
    "\n",
    "def clean_song_title(song_title):\n",
    "    \"\"\" Truncate song title at the first occurrence of '(' or '-' \"\"\"\n",
    "    return re.split(r'\\(|-', song_title)[0].strip()\n",
    "\n",
    "def request_song_url(artist_name, song_title):\n",
    "    \"\"\" Request song URL from Genius API \"\"\"\n",
    "    clean_title = clean_song_title(song_title)\n",
    "    base_url = 'https://api.genius.com'\n",
    "    headers = {'Authorization': 'Bearer ' + GENIUS_API_TOKEN}\n",
    "    search_url = f\"{base_url}/search?q={artist_name} {clean_title}\"\n",
    "    response = requests.get(search_url, headers=headers)\n",
    "    json = response.json()\n",
    "\n",
    "    for hit in json['response']['hits']:\n",
    "        if artist_name.lower() in hit['result']['primary_artist']['name'].lower() and clean_title.lower() in hit['result']['title'].lower():\n",
    "            return hit['result']['url']\n",
    "    return None\n",
    "\n",
    "def scrape_song_lyrics(url):\n",
    "    \"\"\" Scrape song lyrics from Genius URL \"\"\"\n",
    "    if url is None:\n",
    "        return \"URL not found\"\n",
    "    page = requests.get(url)\n",
    "    html = BeautifulSoup(page.text, 'html.parser')\n",
    "    lyrics_div = html.find('div', class_='lyrics') or html.find('div', class_=re.compile(r'Lyrics__Container'))\n",
    "    if lyrics_div:\n",
    "        lyrics = lyrics_div.get_text(strip=True)\n",
    "        lyrics = re.sub(r'[\\(\\[].*?[\\)\\]]', '', lyrics)\n",
    "        lyrics = os.linesep.join([s for s in lyrics.splitlines() if s])\n",
    "    else:\n",
    "        lyrics = \"Lyrics not found\"\n",
    "    return lyrics\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('Top_100_Artists_Across_Top_Genres_With_Random_Song_with_Lyrics.csv')\n",
    "\n",
    "# Process each row to fetch lyrics only for entries marked as needing re-fetching\n",
    "for index, row in data.iterrows():\n",
    "    if row['Lyrics'] == \"New song selected, lyrics need re-fetching\":\n",
    "        url = request_song_url(row['Main Artist'], row['Random Song'])\n",
    "        lyrics = scrape_song_lyrics(url)\n",
    "        data.at[index, 'Lyrics'] = lyrics  # Update the lyrics in the DataFrame\n",
    "\n",
    "# Update the csv file with the new lyrics\n",
    "data.to_csv('Top_100_Artists_Across_Top_Genres_With_Random_Song_with_Lyrics.csv', index=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 Basic Statistics of Textual Data <a class=\"anchor\" id=\"basic_stats_text\"></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text and code "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Artist Collaboration Network Data <a class=\"anchor\" id=\"networkdata\"></a>\n",
    "\n",
    "For the network analysis, we will use the following csv-files created [Data collection from the Spotify API](#spotify): \n",
    "\n",
    "- Final_Feature_Artist_Tracks_Info.csv\n",
    "- Feature_Artist_Songs_Combined.csv\n",
    "- [Complete_Songs_with_Artists_and_Features.csv](#CompleteSongs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.1 Preprocessing and Cleaning <a class=\"anchor\" id=\"networkpreclean\"></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create the final dataset used for the network analysis, we first take *Final_Feature_Artist_Tracks_Info.csv* with the attributes Main Artist, Genre, Popularity Score, and Followers and join on \"Main Artist\" with *Feature_Artist_Songs_Combined.csv* which contains the attributes Song Name, Main Artist and Featured Artist(s). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_artist = pd.read_csv('Artists.csv')\n",
    "\n",
    "df1 = pd.read_csv('Complete_Songs_with_Artists_and_Features.csv', usecols=['Song Name','Main Artist', 'Featured Artists'])\n",
    "\n",
    "merged_df = df1.merge(df_artist, left_on='Main Artist', right_on='Name', how='left')\n",
    "\n",
    "df1_with_data = merged_df.drop(columns=['Name', 'ID', 'URI', 'Age', 'Country', 'Gender'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then join *Complete_Songs_with_Artists_and_Features.csv* with the Kaggle \"US 10k Top Artists and Their Popular Songs\"-dataset on the name of the Main Artist. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('Feature_Artist_Songs_Combined.csv', usecols=['Song Name','Main Artist', 'Featured Artists'])\n",
    "\n",
    "df3 = pd.read_csv('Final_Feature_Artist_Tracks_info.csv', usecols=['Main Artist', 'Genres', 'Popularity', 'Followers'])\n",
    "\n",
    "df2['Main Artist'] = df2['Main Artist'].str.lower()\n",
    "\n",
    "merged_df = df2.merge(df3, on='Main Artist', how='inner')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly we combine the two dataframes, resulting in the final one shown below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_with_data['Main Artist'] = df1_with_data['Main Artist'].str.lower()\n",
    "merged_df = merged_df[~merged_df['Main Artist'].isin(df1_with_data['Main Artist'])]\n",
    "\n",
    "extended_df = pd.concat([df1_with_data, merged_df], ignore_index=True)\n",
    "\n",
    "cleaned_df = extended_df.drop_duplicates()\n",
    "cleaned_df.reset_index(drop=True, inplace=True)\n",
    "cleaned_df.to_csv('Final_Songs_with_Artists_and_Features.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show df \n",
    "cleaned_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2 Creation of Network <a class=\"anchor\" id=\"creation_of_network\"></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the data has been preprocessed and cleaned, we can begin to use the data to construct the .json file containing the Spotify Network. We simply begin with reading the csv file and renaming the genres with the earlier mentioned function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of                                       Song Name Main Artist  \\\n",
      "0                                     One Dance       drake   \n",
      "1       Rich Baby Daddy (feat. Sexyy Red & SZA)       drake   \n",
      "2                            IDGAF (feat. Yeat)       drake   \n",
      "3        act ii: date @ 8 (feat. Drake) - remix       drake   \n",
      "4          First Person Shooter (feat. J. Cole)       drake   \n",
      "...                                         ...         ...   \n",
      "132035                            Bam Bam Bhole  kartik dev   \n",
      "132036          Pyaar Ch Haan (From \"Furteela\")  kartik dev   \n",
      "132037            Main Kamla (From \" Furteela\")  kartik dev   \n",
      "132038                          Satnam Waheguru  kartik dev   \n",
      "132039                               Tu Aaja Na  kartik dev   \n",
      "\n",
      "                             Featured Artists          Genres  Popularity  \\\n",
      "0                                Wizkid, Kyla  [pop, hip hop]          95   \n",
      "1                              Sexyy Red, SZA  [pop, hip hop]          95   \n",
      "2                                        Yeat  [pop, hip hop]          95   \n",
      "3                                       4batz  [pop, hip hop]          95   \n",
      "4                                     J. Cole  [pop, hip hop]          95   \n",
      "...                                       ...             ...         ...   \n",
      "132035        Hansraj Raghuwanshi, Gaurav Dev         [other]          56   \n",
      "132036  Jassie Gill, Gaurav Dev, Raj Fatehpur         [other]          56   \n",
      "132037         Jassie Gill, Gaurav Dev, Abeer         [other]          56   \n",
      "132038                   Gurnazar, Gaurav Dev         [other]          56   \n",
      "132039                   Gurnazar, Stebin Ben         [other]          56   \n",
      "\n",
      "        Followers  \n",
      "0        83298497  \n",
      "1        83298497  \n",
      "2        83298497  \n",
      "3        83298497  \n",
      "4        83298497  \n",
      "...           ...  \n",
      "132035       2411  \n",
      "132036       2411  \n",
      "132037       2411  \n",
      "132038       2411  \n",
      "132039       2411  \n",
      "\n",
      "[132040 rows x 6 columns]>\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('Final_Songs_with_Artists_and_Features.csv')\n",
    "\n",
    "df['Genres'] = df['Genres'].apply(rename_genres) # Split genres by comma\n",
    "\n",
    "print(df.head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afterwards we remove duplicate appearences of main artists, as we are only interested in the characteristics of the artist one time to create the nodes for the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_artists =  df.drop_duplicates(subset='Main Artist') #Only interested in one node per artist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code will address the combinations of genres and put them into groups, where each group corresponds to a set of genres. These are a necessary element of the nodes, as we need to distinguish between the genres when plotting the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of different combiniations of genres: 115\n"
     ]
    }
   ],
   "source": [
    "genres_list  = [row['Genres'] for index, row in df_artists.iterrows()]\n",
    "\n",
    "# Using a Counter to count occurrences of each unique combination\n",
    "# We convert the internal lists to tuples because lists cannot be hashed and used as dictionary keys\n",
    "genre_combinations = Counter(tuple(genre) for genre in genres_list)\n",
    "\n",
    "# Display the count of each unique combination\n",
    "genre_combinations = genre_combinations.most_common()\n",
    "\n",
    "color_mapping = {\n",
    "    'red': '#FF0000',\n",
    "    'green': '#008000',\n",
    "    'blue': '#0000FF',\n",
    "    'yellow': '#FFFF00',\n",
    "    'purple': '#800080',\n",
    "    'orange': '#FFA500',\n",
    "    'teal': '#008080',\n",
    "    'lime': '#00FF00',\n",
    "    'pink': '#FFC0CB',\n",
    "    'grey': '#808080',\n",
    "    'navy': '#000080',\n",
    "    'maroon': '#800000',\n",
    "    'olive': '#808000',\n",
    "    'magenta': '#FF00FF',\n",
    "    'tan': '#D2B48C',\n",
    "    'cyan': '#00FFFF',\n",
    "    'violet': '#EE82EE',\n",
    "    'coral': '#FF7F50',\n",
    "    'khaki': '#F0E68C',\n",
    "    'lavender': '#E6E6FA',\n",
    "    'silver': '#C0C0C0',\n",
    "    'peach': '#FFEFD5',\n",
    "    'bronze': '#CD7F32',\n",
    "    'plum': '#DDA0DD',\n",
    "    'gold': '#FFD700',\n",
    "    'ivory': '#FFFFF0',\n",
    "    'beige': '#F5F5DC',\n",
    "    'mustard': '#FFDB58',\n",
    "    'turquoise': '#40E0D0',\n",
    "    'mint': '#98FF98',\n",
    "    'saffron': '#F4C430',\n",
    "    'fuchsia': '#FF00FF',\n",
    "    'cinnamon': '#D2691E',\n",
    "    'chartreuse': '#7FFF00',\n",
    "    'azure': '#F0FFFF',\n",
    "    'indigo': '#4B0082',\n",
    "    'periwinkle': '#CCCCFF',\n",
    "    'sand': '#C2B280',\n",
    "    'clay': '#B66D28',\n",
    "    'honeydew': '#F0FFF0',\n",
    "    'flamingo': '#FC8EAC',\n",
    "    'lemon': '#FFFACD',\n",
    "    'raspberry': '#E30B5D',\n",
    "    'forest green': '#228B22',\n",
    "    'auburn': '#A52A2A',\n",
    "    'midnight blue': '#191970',\n",
    "    'cream': '#FFFDD0',\n",
    "    'amber': '#FFBF00',\n",
    "    'emerald': '#50C878',\n",
    "    'sapphire': '#0F52BA',\n",
    "    'cerulean': '#007BA7',\n",
    "    'charcoal': '#36454F',\n",
    "    'ruby': '#E0115F',\n",
    "    'slate': '#708090',\n",
    "    'orchid': '#DA70D6',\n",
    "    'crimson': '#DC143C',\n",
    "    'cobalt': '#0047AB',\n",
    "    'ocean blue': '#4F42B5',\n",
    "    'firebrick': '#B22222',\n",
    "    'moss green': '#8A9A5B',\n",
    "    'jade': '#00A86B',\n",
    "    'eggplant': '#614051',\n",
    "    'copper': '#B87333',\n",
    "    'cherry': '#DE3163',\n",
    "    'sepia': '#704214',\n",
    "    'citrine': '#E4D00A',\n",
    "    'gunmetal': '#2a3439',\n",
    "    'pistachio': '#93C572',\n",
    "    'salmon': '#FA8072',\n",
    "    'caramel': '#FFD59A',\n",
    "    'taupe': '#483C32',\n",
    "    'electric blue': '#7DF9FF',\n",
    "    'mauve': '#E0B0FF',\n",
    "    'sienna': '#882D17',\n",
    "    'fern green': '#4F7942',\n",
    "    'butter': '#FFF48D',\n",
    "    'tangerine': '#F28500',\n",
    "}\n",
    "category_dict = {}\n",
    "cat_tracker = 0\n",
    "for genres, count in genre_combinations:\n",
    "    if genres not in category_dict and count > 100:\n",
    "        color = list(color_mapping.keys())[cat_tracker]\n",
    "        category_dict[genres] = color_mapping[color]\n",
    "        cat_tracker += 1\n",
    "    if genres not in category_dict and count <= 100:\n",
    "        category_dict[genres] = '#000000'\n",
    "    if cat_tracker >= len(color_mapping):\n",
    "        cat_tracker = 0\n",
    "\n",
    "print(\"Number of different combiniations of genres:\", len(category_dict))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen by the print, there are 115 different combinations of genres. This is too ambiguous to picture in any meaningful way if we were to plot the network. Therefore we greedily reduce the number of genres by picking all the genres that has atleast more than 200 artists within it. Moreover, we needed to include all the predefined genres or else they would not make the cut.\n",
    "\n",
    "The following code will specifically select genres based on the former mentioned criteria and put them into a list, as well as a dictionary that will be used later in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Combinations: [('other',), ('pop',), ('hip hop',), ('pop', 'hip hop'), ('r&b',), ('electronic',), ('rock',), ('latin',), ('pop', 'rock'), ('pop', 'electronic'), ('hindi/bollywood',), ('indie',), ('reggae',), ('hollywood',), ('country',)]\n"
     ]
    }
   ],
   "source": [
    "top_combinations = []\n",
    "top_combinations_dict = {}\n",
    "for genre, count in genre_combinations:\n",
    "    if count > 200 or len(genre) == 1:\n",
    "        top_combinations_dict[genre] = category_dict[genre]\n",
    "        top_combinations.append(genre)\n",
    "\n",
    "print(\"Final Combinations:\", top_combinations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we needed to actually reduce the number of combinations, the following segment of code does that. However, it should be mentioned that the method is rather rudimentory, as it reassign the genres when a combination within the top combinations is a subset of the genres. This could have been implemented with much more sufficient methods, as we will touch more upon in the discussion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for genres, count in reversed(genre_combinations):\n",
    "    if count <= 200: # Should not handle new top combinations\n",
    "        for combination in top_combinations:\n",
    "            if(all(x in genres for x in combination)):\n",
    "                category_dict[genres] = category_dict[combination]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point all the attributes for the nodes have been prepared, thus we loop through each artist create a list containing tuples of `(artist_name, artist_attributes)` for every artist within the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add attributes for each artist\n",
    "attr_list = []\n",
    "for index, row in tqdm.tqdm(df_artists.iterrows(), total=df_artists.shape[0]):\n",
    "    artist_attributes = {'genres': row['Genres'], 'popularity': row['Popularity'], 'followers': row['Followers'], 'size': 1, 'group' : category_dict[tuple(row['Genres'])]}\n",
    "    attr_list.append((row['Main Artist'], artist_attributes))\n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus we have established information for every node within the network. So, we can move onto the edges.\n",
    "\n",
    "We need two helper functions to handle the dataset. Specifically one function called `aggregate_features` that collects all the features across the top 10 tracks of an artist and collects them into one single list, and another function called `lower` that transforms each feature to lower-case, or else they would not match with artists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_features(x):\n",
    "    flat_list = [item.strip() for sublist in x for item in sublist.split(',')]\n",
    "    return list(flat_list)\n",
    "\n",
    "def lower(list):\n",
    "    return [x.lower() for x in list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section of the code removes rows where a song has no features, then collects each row of features for one artists into a single row, and lastly, transforms each feature to lower-case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_artists_features = df.dropna() \n",
    "\n",
    "df_grouped_featured = df_artists_features.groupby('Main Artist')['Featured Artists'].agg(aggregate_features).reset_index()\n",
    "\n",
    "\n",
    "df_grouped_featured['Featured Artists'] = df_grouped_featured['Featured Artists'].apply(lower) \n",
    "\n",
    "# # Save the result to a CSV file\n",
    "# df_grouped_featured.to_csv('Artists_Features.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we collect the edges of our dataset. Collabs is a list where each element is a tuple that consists of a main artist and one of its features. The connection is only established if the featured artist is within the confines of the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collabs = []\n",
    "for artist_idx in range(len(df_grouped_featured['Main Artist'])):\n",
    "    features = df_grouped_featured['Featured Artists'][artist_idx]\n",
    "    main_artist = df_grouped_featured['Main Artist'][artist_idx]\n",
    "    collabs.extend((main_artist, feature) for feature in features if feature in df_artists_features['Main Artist'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the former code, we will now collect the amount of times two artists have collaborated and create an undirected edge between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count the number of occurences of the Collaborations\n",
    "counter = Counter(collabs)\n",
    "occurrences = counter.most_common()\n",
    "artists_collabs = [(pair[0], pair[1], count) for pair, count in occurrences]\n",
    "artists_edges = [{'source': pair[0], 'target': pair[1], 'value' : count} for pair, count in occurrences]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the nodes and edges have now been collected, therefore we can continue with populating the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create graph and employ the G.add_weighted_edges_from() function\n",
    "G = nx.Graph()\n",
    "for artist_name, attr in attr_list:\n",
    "    G.add_node(artist_name, genres = attr['genres'], popularity = attr['popularity'], followers = attr['followers'], size = attr['size'], group = attr['group'])\n",
    "G.add_weighted_edges_from(artists_collabs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For later visualizations, we will scale the size of the nodes according to their followers. We found this to be a better way to represent the popularity as the size of the nodes compared to using the \"Popularity\" or the degrees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale size of nodes by followers\n",
    "for node in G.nodes():\n",
    "    G.nodes[node]['size'] = G.nodes[node]['followers']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section creates a legend for the `top_combinations` list, and it will later be used for when the network is plottet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "patches = [mpatches.Patch(color=color, label=', '.join(genres)) for genres, color in top_combinations_dict.items()]\n",
    "legend = ax.legend(handles=patches, loc='center', frameon=False)\n",
    "ax.axis('off')\n",
    "\n",
    "# Save or display the legend\n",
    "plt.savefig('legend.png') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we export the graph as .json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_as_json = json_graph.node_link_data(G)\n",
    "\n",
    "with open('graph2.json', 'w') as f:\n",
    "    json.dump(graph_as_json, f, indent=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.3 Basic Statistics - Network Data <a class=\"anchor\" id=\"basic_stats_network\"></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Analysis <a class=\"anchor\" id=\"data_analysis\"></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Network Analysis <a class=\"anchor\" id=\"network_analysis\"></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1 Degree Distribution <a class=\"anchor\" id=\"degree_distribution\"></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2 Assortivity <a class=\"anchor\" id=\"assortivity\"></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.3 Communities <a class=\"anchor\" id=\"communities\"></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Textual Analysis <a class=\"anchor\" id=\"text_analysis\"></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 Motivation for Textual Analysis <a class=\"anchor\" id=\"motivation_text\"></a>\n",
    "\n",
    "Textual analysis of song lyrics is used in our project to investigate distinctive themes and language patterns across different genres. This approach is interesting for several reasons:\n",
    "\n",
    "**Genre-Specific Themes**: We would like to see if it is possible to identify genre-specific characteristics/themes. Analyzing these differences should help to define what sets the genres apart.\n",
    "\n",
    "**Comparative Analysis**: By looking into the frequency and context of words within genres, we can identify which themes and words are predominant for each genre. This analysis helps shed light on what characterises the different genres."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2 TF-IDF <a class=\"anchor\" id=\"TF_IDF\"></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.3 Wordclouds <a class=\"anchor\" id=\"wordclouds\"></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Discussion <a class=\"anchor\" id=\"discussion\"></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. References <a class=\"anchor\" id=\"references\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
