{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a93c99e0-22aa-4247-9d61-4f162eb930d1",
   "metadata": {},
   "source": [
    "## Assignment 1\n",
    "### Group 14 \n",
    "\n",
    "> Github repository: https://github.com/LivDreyer/CSS24.git\n",
    "\n",
    "> Shortlog of git commits:\n",
    "- 7  LivDreyer\n",
    "- 3  AIAndreas\n",
    "- 2  FelixxAI\n",
    "\n",
    "The git-commit is a little skewed as some files were sent privately and not commited through the github. Addtionally, some exercises used to generate .csv-files were not included in this assignment meaning they were also not commited.\n",
    "\n",
    "> Contribution: The workload was distributed equally between all members of the group. \n",
    "\n",
    "> Data: The data has been attached in a zip-file with corresponding names for the data loading in this file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "88b9e9fb-277c-4d83-9a3b-75b6664ee679",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importations \n",
    "import requests \n",
    "from bs4 import BeautifulSoup as BS\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import ast\n",
    "import networkx as nx\n",
    "import matplotlib\n",
    "import requests\n",
    "from datetime import datetime\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import time\n",
    "from collections import Counter\n",
    "import json\n",
    "from statistics import mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2049e25-96ed-4ce9-80d5-d058adfd7349",
   "metadata": {},
   "source": [
    "## Part 1: Web-scrabing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d3b36f-6ce6-4fdf-b686-c8082b8fefde",
   "metadata": {},
   "source": [
    "In this part, we have included keynote speakers, chairs, authors of parallel talks, authors of posters, the names of researchers from the programme committee of the conference, and the organizers of tutorials. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1228110-abf3-4875-b69f-dbaef8928520",
   "metadata": {},
   "source": [
    "#### Names from the program: Plenary Speakers, Keynote Speakers, Authors of Parallel Talks, Authors of Posters, and Chairs\n",
    "#### Link: https://ic2s2-2023.org/program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e8f67921-69f9-4beb-b725-d46f9769c6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get html to py\n",
    "\n",
    "link = \"https://ic2s2-2023.org/program\"\n",
    "r = requests.get(link)\n",
    "soup = BS(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5b3f21a3-8e2e-41f3-bb18-cb74fe0eefa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plenary, parallel talks, posters speaker\n",
    "# Name of final list: ppp_names\n",
    "\n",
    "#find programs from each day \n",
    "plen = soup.find_all(\"ul\", {\"class\":\"nav_list\"})\n",
    "\n",
    "#find all names from the day \n",
    "names_plen = []\n",
    "names_plen_text = []\n",
    "for i in range(len(plen)): \n",
    "    names_plen.append(plen[i].find_all(\"i\"))\n",
    "\n",
    "for i in range(len(names_plen)):\n",
    "    for j in range(len(names_plen[i])):\n",
    "        names_plen_text.append(names_plen[i][j].text)\n",
    "\n",
    "plen_names_lst = [name for names in names_plen_text for name in names.split(', ')]\n",
    "\n",
    "ppp_names = []\n",
    "for i in range(len(plen_names_lst)): \n",
    "    ppp_names.append(plen_names_lst[i].title())\n",
    "\n",
    "\n",
    "for name in ppp_names:\n",
    "    if '\\u201a' in name:\n",
    "        ppp_names.remove(name)\n",
    "\n",
    "#print(ppp_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "be2eb25d-3e1e-4b81-8e72-920d53c44324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keynote speakers \n",
    "# Name of final list: key_names\n",
    "\n",
    "table = soup.find_all(\"table\", {\"class\": \"tutorials\"})\n",
    "table = table[0].find_all(\"td\")\n",
    "\n",
    "table_text = []\n",
    "for i in range(len(table)):\n",
    "    txt = table[i].text\n",
    "    if \"Keynote\" in txt: \n",
    "        table_text.append(txt)\n",
    "\n",
    "key_names = []\n",
    "for i in table_text: \n",
    "    key_names.append(i[10:])\n",
    "\n",
    "\n",
    "#print(key_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "308d7707-ef51-4ae9-abaa-c8c6a691694a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chairs\n",
    "# Name of final list: chair_names\n",
    "\n",
    "\n",
    "chair = soup.find_all(\"div\", {\"class\":\"col-9 col-12-medium\"})\n",
    "\n",
    "#find chairs\n",
    "names_chairs = []\n",
    "names_chairs_text = []\n",
    "for i in range(len(chair)): \n",
    "    names_chairs.append(chair[i].find_all(\"h2\"))\n",
    "\n",
    "for i in range(len(names_chairs)):\n",
    "    for j in range(len(names_chairs[i])):\n",
    "        names_chairs_text.append(names_chairs[i][j].text)\n",
    "\n",
    "\n",
    "chr = []\n",
    "\n",
    "for i in names_chairs_text:\n",
    "    if \"Chair\" in i: \n",
    "        chr.append(i)\n",
    "\n",
    "chair_names = []\n",
    "for i in chr: \n",
    "    idx = i.find(\"Chair:\")\n",
    "    chair_names.append(i[idx + len(\"Chair:\"):].strip())\n",
    "    \n",
    "    \n",
    "#print(chair_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bae853-870f-4a0d-8715-f304c1e3c18f",
   "metadata": {},
   "source": [
    "#### Names from the programme committee \n",
    "#### Link: https://ic2s2-2023.org/program_committee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "dae3a5b1-2d42-482d-8598-486a9752d7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "link_pc = \"https://ic2s2-2023.org/program_committee\"\n",
    "r_pc = requests.get(link_pc)\n",
    "soup_pc = BS(r_pc.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "860ff4b1-b313-4481-83c4-f2fd70836ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Programme Committee \n",
    "# Name of final list: pcom_names \n",
    "\n",
    "pc = soup_pc.find_all(\"article\", {\"class\":\"box post\"})\n",
    "\n",
    "#find all names from the day \n",
    "pc_names = []\n",
    "pc_text = []\n",
    "for i in range(len(pc)): \n",
    "    pc_names.append(pc[i].find_all(\"li\"))\n",
    "\n",
    "for i in range(len(pc_names)):\n",
    "    for j in range(len(pc_names[i])):\n",
    "        pc_text.append(pc_names[i][j].text)\n",
    "\n",
    "\n",
    "pcom_names = []\n",
    "for i in range(len(pc_text)):\n",
    "    lst = pc_text[i].split(\" (\")\n",
    "    pcom_names.append(lst[0])\n",
    "\n",
    "#print(pcom_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfbc99c-2512-43c1-b134-cb3130f252d2",
   "metadata": {},
   "source": [
    "#### Names from the Tutorials\n",
    "#### Link: https://ic2s2-2023.org/tutorials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "87229e41-7f5c-45ba-aa9d-16f75c6f78ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "link_t = \"https://ic2s2-2023.org/tutorials\"\n",
    "r_t = requests.get(link_t)\n",
    "soup_t = BS(r_t.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e8ed48a0-0b43-4ed1-a975-27fdef56cd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tutorials \n",
    "# Name of final list: tut_names_f\n",
    "\n",
    "tut = soup_t.find_all(\"div\", {\"class\": \"col-5 col-12-medium\"})\n",
    "\n",
    "#find all names from the day \n",
    "tut_names = []\n",
    "tut_text = []\n",
    "for i in range(len(tut)): \n",
    "    tut_names.append(tut[i].find_all(\"li\"))\n",
    "\n",
    "for i in range(len(tut_names)):\n",
    "    for j in range(len(tut_names[i])):\n",
    "        tut_text.append(tut_names[i][j].text)\n",
    "\n",
    "#print(tut_text)\n",
    "\n",
    "tut_names_f = []\n",
    "for i in range(len(tut_text)):\n",
    "    lst = tut_text[i].split(\",\")\n",
    "    tut_names_f.append(lst[0])\n",
    "\n",
    "#print(tut_names_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5faf36ea-c00d-4b11-94e3-afb5cea807a1",
   "metadata": {},
   "source": [
    "### Collect all names in one final string and process "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "828578ac-19c3-4a6b-bd2a-3c599d8cac81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of names including optional exercises:  1620\n"
     ]
    }
   ],
   "source": [
    "# Collect all names in one list of strings \n",
    "\n",
    "all_names = chair_names + key_names + ppp_names + pcom_names + tut_names_f\n",
    "#print(all_names)\n",
    "\n",
    "#Capitalize every name \n",
    "all_names_final = []\n",
    "for i in range(len(all_names)): \n",
    "    all_names_final.append(all_names[i].title())\n",
    "\n",
    "all_names_final = list(set(all_names_final))\n",
    "\n",
    "\n",
    "#Pandas DF \n",
    "names_DF = pd.DataFrame(all_names_final, columns=['Researcher Name'])\n",
    "names_DF = names_DF.sort_values(by='Researcher Name')\n",
    "\n",
    "#Split names into first name and last name, dropping middle names in all formats: \"John Name Doe\" or \"John N. Doe\"\n",
    "names_DF['first_name'] = names_DF['Researcher Name'].str.split().str[0]\n",
    "names_DF['last_name'] = names_DF['Researcher Name'].str.split().str[-1]\n",
    "\n",
    "#Remove duplicates of names such as \"John Name Doe\" or \"John N. Doe\"\n",
    "names_DF.drop_duplicates(subset=['first_name', 'last_name'], inplace=True)\n",
    "\n",
    "#Save first column (names) to file\n",
    "names_DF['Researcher Name'].to_csv('researchers_names.csv', index=False)\n",
    "\n",
    "#Find number of researchers\n",
    "\n",
    "print(\"Number of names including optional exercises: \",len(names_DF))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fb5ce2-3445-4400-8b8b-055b30d5d64f",
   "metadata": {},
   "source": [
    "**How many unique researchers did you get?**\n",
    "\n",
    "As seen above, we got 1620 unique names of researchers when including the optional exercises. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7c4e98-f138-497c-883b-0044e50c5640",
   "metadata": {},
   "source": [
    "**Explain the process you followed to web-scrape the page. Which choices did you make to accurately retreive as many names as possible? Which strategies did you use to assess the quality of your final list? Explain your reasoning and your choices (answer in max 150 words).**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a46001-f5fb-4cf5-9f7a-638e64874c3e",
   "metadata": {},
   "source": [
    "When initially inspecting the webpage to understand how it was structured, and where we should look, we relied on visual cues signaling what html-code corresponded to the data we wanted. Working with retrieving names of researchers from the program, we structured the retrieval in three parts, as we recognized different patterns for each type of name. We repeated this process for the optional exercise. For each part we used the [BeautifulSoup Python package](https://pypi.org/project/beautifulsoup4/)'s .[find_all](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#find-all) method to extract the wanted names, and lastly combined every list of names to one final list. For the final list, we capitalized each name, and then used the set() method to remove apparent duplicates. We then created a Pandas Dataframe, where we split every name into first and last name excluding the middle name. It was then possible to remove rows with identical first and last name, letting us create a csv.file with unique “Researcher Names”."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f4333c-00da-4ef8-9d02-1b639c909964",
   "metadata": {},
   "source": [
    "## Part 2: Ready Made vs Custom Made Data\n",
    "\n",
    "*Exercise: Ready made data vs Custom made data In this exercise, I want to make sure you have understood they key points of my lecture and the reading.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01dbd011-835b-437f-add5-4e94667adfda",
   "metadata": {},
   "source": [
    "**What are pros and cons of the custom-made data used in Centola's experiment (the first study presented in the lecture) and the ready-made data used in Nicolaides's study (the second study presented in the lecture)? You can support your arguments based on the content of the lecture and the information you read in Chapter 2.3 of the book (answer in max 150 words).**\n",
    "\n",
    "Centola’s experiment [1] allows the investigation of diffusion processes with no influence from confounding variables such as advertising or social reinforcement from outside of the experiment. Though generally desired it’s hard to apply the results directly on real-word settings. Despite a smaller sample size, Centola could repeat the experiment to reproduce tendencies of spread of information. A problem of volunteer bias could however possibly arise from how he recruited participants.\n",
    "In Nicolaides’s study [2] the large sample size and the non-reactivity of users constitute an advantage when investigating contagion as it can mirror real-world conditions. However, as mentioned in “Bit by bit” 2.3.3 [3], non-reactivity does not ensure that the data directly reflects behavior. Users of the fitness-app is not forced to participate even though they have connections on the app, and to paraphrase from the book: “It’s not that I am not exercising, I am just not posting it on thefitness- app”. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51fea8d0-8ec6-4db8-a2e5-19a4c5a22d01",
   "metadata": {},
   "source": [
    "**How do you think these differences can influence the interpretation of the results in each study? (answer in max 150 words)**\n",
    "\n",
    "Centola’s findings are based on an artificial environment that find significant results mapping the “essence” of behavioral contagion, however, the reactivity of participants could influence the interpretation of the results. Additionally, as mentioned, the participants were recruited from health-sites which could promote the behavior Centola finds considering his experiment is also concerned with health behavior. \n",
    "\n",
    "Depending on how Nicolaides performs the linear regression to examine how one user’s exercise might influence their connections’ exercise on the fitness-tracker app, it might not mirror the actual contagion happening. If people are active users who follow their friends, but do not post their exercise themselves, the results of the linear regression may reflect this more than the actual influence users have on their connections.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f529d25e-7185-433f-9791-e582c2d7bdbe",
   "metadata": {},
   "source": [
    "[1] Damon Centola ,The Spread of Behavior in an Online Social Network Experiment.Science329,1194-1197(2010).DOI:10.1126/science.1185231\n",
    "\n",
    "[2] Aral S, Nicolaides C. Exercise contagion in a global social network. Nat Commun. 2017 Apr 18;8:14753. doi: 10.1038/ncomms14753.\n",
    "\n",
    "[3] Salganik, Matthew J. 2017. Bit by Bit: Social Research in the Digital Age. Princeton, NJ: Princeton University Press. Open review edition."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b945e157-2ddc-4340-9888-80f507371ed7",
   "metadata": {},
   "source": [
    "## Part 3: Gathering Research Articles using the OpenAlex API"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "016789ff",
   "metadata": {},
   "source": [
    "### Step 1: Retrieve data. \n",
    "\n",
    "**Starting with the authors you identified in Week 2, Exercise 2, use the OpenAlex API works endpoint to fetch the research articles they have authored. For each article, retrieve the following details:**\n",
    "\n",
    "- _id_: The unique OpenAlex ID for the work.\n",
    "- _publication_year_: The year the work was published.\n",
    "- _cited_by_count_: The number of times the work has been cited by other works.\n",
    "- _author_ids_: The OpenAlex IDs for the authors of the work.\n",
    "- _title_: The title of the work.\n",
    "- _abstract_inverted_index_: The abstract of the work, formatted as an inverted index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1dff73d2-63fb-4aee-8cde-61b45624fd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "SOCIAL_SCIENCE_CONCEPTS = {'https://openalex.org/C144024400', 'https://openalex.org/C15744967', 'https://openalex.org/C162324750', 'https://openalex.org/C17744445'}\n",
    "QUANTITATIVE_CONCEPTS = {'https://openalex.org/C33923547', 'https://openalex.org/C121332964', 'https://openalex.org/C41008148'}\n",
    "\n",
    "# Session for connection pooling and retries\n",
    "session = requests.Session()\n",
    "retries = Retry(total=5, backoff_factor=1, status_forcelist=[429])\n",
    "session.mount('https://', HTTPAdapter(max_retries=retries))\n",
    "\n",
    "def work_meets_criteria(work):\n",
    "    social_science_present = any(concept['id'] in SOCIAL_SCIENCE_CONCEPTS for concept in work.get('concepts', []))\n",
    "    quantitative_present = any(concept['id'] in QUANTITATIVE_CONCEPTS for concept in work.get('concepts', []))\n",
    "    return social_science_present and quantitative_present\n",
    "\n",
    "def fetch_work_for_author(author_id):\n",
    "    works_details = []\n",
    "    abstracts_details = []\n",
    "    cursor = '*'\n",
    "    author_id_query = f\"author.id:{author_id}\"\n",
    "    citations_filter = \"cited_by_count:>10\"\n",
    "\n",
    "    while cursor:\n",
    "        url = f\"https://api.openalex.org/works?filter={author_id_query},{citations_filter}&per-page=200&cursor={cursor}\"\n",
    "        response = session.get(url, timeout=10)\n",
    "        data = response.json()\n",
    "        works = data.get('results', [])\n",
    "        for work in works:\n",
    "            if work_meets_criteria(work):\n",
    "                author_ids = [authorship['author']['id'] for authorship in work.get('authorships', []) if 'author' in authorship and 'id' in authorship['author']]\n",
    "                if len(author_ids) < 10:\n",
    "                    works_details.append({\n",
    "                        'id': work['id'],\n",
    "                        'publication_year': work.get('publication_year'),\n",
    "                        'cited_by_count': work.get('cited_by_count'),\n",
    "                        'title': work.get('title'),\n",
    "                        'author_ids': ';'.join(author_ids)\n",
    "                    })\n",
    "                    abstracts_details.append({\n",
    "                        'id': work['id'],\n",
    "                        'title': work.get('title'),\n",
    "                        'abstract_inverted_index': work.get('abstract_inverted_index')\n",
    "                    })\n",
    "        cursor = data.get('meta', {}).get('next_cursor')\n",
    "    return works_details, abstracts_details\n",
    "\n",
    "def fetch_works_and_abstracts_for_authors(author_ids):\n",
    "    works_details = []\n",
    "    abstracts_details = []\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor: \n",
    "        futures = [executor.submit(fetch_work_for_author, author_id) for author_id in author_ids]\n",
    "        for future in as_completed(futures):\n",
    "            author_works, author_abstracts = future.result()\n",
    "            works_details.extend(author_works)\n",
    "            abstracts_details.extend(author_abstracts)\n",
    "    return works_details, abstracts_details\n",
    "\n",
    "def parallel_fetch_works_and_abstracts(author_ids, batch_size=20):  \n",
    "    papers_data = []\n",
    "    abstracts_data = []\n",
    "    with ThreadPoolExecutor(max_workers=5) as executor:  \n",
    "        for i in range(0, len(author_ids), batch_size):\n",
    "            author_ids_batch = author_ids[i:i+batch_size]\n",
    "            futures = [executor.submit(fetch_works_and_abstracts_for_authors, author_ids_batch)]\n",
    "            for future in as_completed(futures):\n",
    "                works_details, abstracts_details = future.result()\n",
    "                papers_data.extend(works_details)\n",
    "                abstracts_data.extend(abstracts_details)\n",
    "    return papers_data, abstracts_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23faf5df-f1a4-42d4-9e52-ec7ee7256c34",
   "metadata": {},
   "source": [
    "**Data Storage:** Organize the retrieved information into two Pandas DataFrames and save them to two files in a suitable format:\n",
    "- The *IC2S2 papers* dataset should include: *id, publication\\_year, cited\\_by\\_count, author\\_ids*.\n",
    "- The *IC2S2 abstracts* dataset should include: *id, title, abstract\\_inverted\\_index*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "68b560b0-0025-450c-94d9-a381657e4028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your author IDs here\n",
    "researchers_df = pd.read_csv(\"IC2S2_authors_dataset.csv\")\n",
    "author_ids_filtered = researchers_df[(researchers_df['works_count'] >= 5) & (researchers_df['works_count'] <= 5000)]['id'].apply(lambda x: x.split('/')[-1]).tolist()\n",
    "\n",
    "# Fetch works and abstracts\n",
    "papers_data, abstracts_data = parallel_fetch_works_and_abstracts(author_ids_filtered)\n",
    "\n",
    "# Convert to DataFrame and save\n",
    "papers_df = pd.DataFrame(papers_data)\n",
    "abstracts_df = pd.DataFrame(abstracts_data)\n",
    "papers_df.to_csv('IC2S2_papers_dataset.csv', index=False)\n",
    "abstracts_df.to_csv('IC2S2_abstracts_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf5f7fe-3739-4912-82ea-714de5384827",
   "metadata": {},
   "source": [
    "**Data Overview and Reflection questions:** Answer the following questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2c8ba9-24f9-4b9b-9cab-6689e7164a9d",
   "metadata": {},
   "source": [
    "**1. Dataset summary.** How many works are listed in your IC2S2 papers dataframe? How many unique researchers have co-authored these works?\n",
    "\n",
    "Number of works listed in IC2S2 papers dataframe: 10509\n",
    "Number of unique researchers who have co-authored these works: 14095\n",
    "\n",
    "Please see the code below as well: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b0094a51-1b32-4109-af29-b6500c37ac6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of works listed in IC2S2 papers dataframe: 10527\n",
      "Number of unique researchers who have co-authored these works: 14115\n"
     ]
    }
   ],
   "source": [
    "# Dataset summary\n",
    "num_works = len(papers_df)\n",
    "num_unique_researchers = len(set(';'.join(papers_df['author_ids']).split(';')))\n",
    "print(f\"Number of works listed in IC2S2 papers dataframe: {num_works}\")\n",
    "print(f\"Number of unique researchers who have co-authored these works: {num_unique_researchers}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc839808-7acd-4d06-8a9d-7298e86930d9",
   "metadata": {},
   "source": [
    "**2. Efficiency in code.** Describe the strategies you implemented to make your code more efficient. How did your approach affect your code's execution time? (answer in max 150 words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b01bfc-dc81-4785-94df-cb613c7d6923",
   "metadata": {},
   "source": [
    "To enhance the efficiency of the code we implemeted a retry mechanism as we kept running into error code 429 which we later learned was too many requests which was a byproduct of the ThreadPoolExecutor. Beacuse of this we ended up also implementing a backoff period which was done to make the requests stop for a moment so we didnt keep running into the 10 request per second hard cap. We also implemented batch requests to streamline the process which also reduced the occurence of error code 429."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e205962-8cb6-4f03-8a97-7acfabb5cf33",
   "metadata": {},
   "source": [
    "**3. Filtering Criteria and Dataset Relevance** \n",
    "Reflect on the rationale behind setting specific thresholds for the total number of works by an author, the citation count, the number of authors per work, and the relevance of works to specific fields. How do these filtering criteria contribute to the relevance of the dataset you compiled? Do you believe any aspects of Computational Social Science research might be underrepresented or overrepresented as a result of these choices? (answer in max 150 words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bc6980-829e-49b1-b947-c6376754e9b7",
   "metadata": {},
   "source": [
    "Setting thresholds for total works, citation count, and number of authors per work ensures that only the most relevant works and authors are included. Take for example the citation count, in the acedemic community citation count is almost directly correlated with relevance and is a sort of unofficial quality-checker. Including works with less than 10 citations risks bringing in works of questionable quality.\n",
    "The filtering of papers not tagged with one of the relevant fields of study is kind of self explanatory as we do not want non-relevant fields of study in the paper pool. These filters make sure that we do not see an over-representation of non-relevant papers which in the end should be beneficial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985eae0c",
   "metadata": {},
   "source": [
    "## Part 4: The Network of Computational Social Scientists"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2e2674-1508-4714-bbb4-594fa096b652",
   "metadata": {},
   "source": [
    "### Part 1: Network Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7b458b-d498-4ec4-8b1f-85211236f87e",
   "metadata": {},
   "source": [
    "**Weighted Edgelist Creation:** Start with your dataframe of papers. Construct a weighted edgelist where each list element is a tuple containing three elements: the author ids of two collaborating authors and the total number of papers they've co-authored. Ensure each author pair is listed only once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d51ed386-b448-41a3-aba1-0ad64aca1976",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load dataframes \n",
    "\n",
    "\n",
    "papers_df = pd.read_csv('final_papers_dataset.csv', usecols = [\"id\",\"author_ids\",\"cited_by_count\",\"publication_year\"])\n",
    "authors_df = pd.read_csv('final_authors_dataset.csv', usecols = [\"id\",\"display_name\", \"country_code\"])\n",
    "\n",
    "papers_df['author_ids'] = papers_df['author_ids'].apply(lambda x: [url if url.startswith('https://openalex.org/') else 'https://openalex.org/' + url for url in ast.literal_eval(x)] if x.startswith('[') else ['https://openalex.org/' + url if not url.startswith('https://openalex.org/') else url for url in x.split(';')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ddd7ab54-1e14-4dc4-95c2-4ff9de2755a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n"
     ]
    }
   ],
   "source": [
    "#Create List of Collaborations between authors\n",
    "\n",
    "collabs = []\n",
    "unique_authors = set()\n",
    "for work_idx in range(len(papers_df[\"id\"])):\n",
    "    work = papers_df[\"id\"][work_idx]\n",
    "    collabs.extend([(a, b) for idx, a in enumerate(papers_df[\"author_ids\"][work_idx]) for b in papers_df[\"author_ids\"][work_idx][idx + 1:]])\n",
    "    unique_authors.update(papers_df[\"author_ids\"][work_idx])\n",
    "    if work_idx % 10000 == 0:\n",
    "        print(work_idx) #To keep track of where we are in the process\n",
    "\n",
    "\n",
    "#Count the number of occurences of the Collaborations\n",
    "counter = Counter(collabs)\n",
    "occurrences = counter.most_common()\n",
    "authors_collabs = [(pair[0], pair[1], count) for pair, count in occurrences]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a14a58b-572d-44d2-a51e-599ab3a12fa8",
   "metadata": {},
   "source": [
    "**Graph Construction:**\n",
    "\n",
    "- Use NetworkX to create an undirected Graph.\n",
    "- Employ the add_weighted_edges_from function to populate the graph with the weighted edgelist from step 1, creating a weighted, undirected graph."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5878c5f4-727e-4cbe-94d0-fd058010dd0e",
   "metadata": {},
   "source": [
    "**Node Attributes:**\n",
    "\n",
    "- For each node, add attributes for the author's display name, country, citation count, and the year of their first publication in Computational Social Science. The display name and country can be retrieved from your authors dataset. The year of their first publication and the citation count can be retrieved from the papers dataset.\n",
    "- Save the network as a JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e1174767-6fed-41e1-9492-3ac7a71bccc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n"
     ]
    }
   ],
   "source": [
    "#Create dataframe for graph analysis\n",
    "authors = authors_df[authors_df['id'].isin(unique_authors)]\n",
    "authors_citations = []\n",
    "idx = 0\n",
    "\n",
    "for author_id in authors['id']:\n",
    "\n",
    "    author_papers = papers_df[papers_df['author_ids'].apply(lambda x: author_id in x)]\n",
    "    total_citations = author_papers['cited_by_count'].sum()\n",
    "    first_publication_year = author_papers['publication_year'].min()\n",
    "    authors_citations.append((author_id, total_citations, first_publication_year))\n",
    "    idx += 1\n",
    "    if idx % 1000 == 0: #To keep track of where we are in the process\n",
    "        print(idx)\n",
    "\n",
    "authors_citations_df = pd.DataFrame(authors_citations, columns=['Author ID', 'Total Citations', 'First Publication Year'])\n",
    "combined_df = pd.merge(authors, authors_citations_df, left_on='id', right_on='Author ID')\n",
    "combined_df = combined_df[['Author ID', 'display_name', 'country_code', 'Total Citations', 'First Publication Year']]\n",
    "combined_df.columns = ['Author ID', 'display_name', 'country', 'citation_count', 'first_publication']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "8cd20fa5-e2d3-4c33-a196-4daf91d67275",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add attributes for each author to list \n",
    "\n",
    "tupleattr_list = []\n",
    "for i in range(len(combined_df)):\n",
    "    id = combined_df['Author ID'][i]\n",
    "    author_attribute = {'display_name': combined_df['display_name'][i], 'country': combined_df['country'][i], 'citation_count': int(combined_df['citation_count'][i]), 'first_publication': int(combined_df['first_publication'][i])}\n",
    "    tupleattr_list.append((id, author_attribute))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c47ddc43-ee9a-4faa-af3b-a0195bb206e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create graph and employ the G.add_weighted_edges_from() function\n",
    "G = nx.Graph()\n",
    "G.add_nodes_from(tupleattr_list)\n",
    "\n",
    "#Check that we have the correct other before creating edges\n",
    "authors_ids = set(authors['id'])\n",
    "authors_collabs = [(a, b, count) for a, b, count in authors_collabs if a in authors_ids and b in authors_ids]\n",
    "\n",
    "G.add_weighted_edges_from(authors_collabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "3b5933d9-266c-43e6-8397-d93008bd1a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the network as a JSON file\n",
    "data = nx.node_link_data(G)\n",
    "\n",
    "with open('graph.json', 'w') as f:\n",
    "    json.dump(data, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c483d841-fcc0-410d-b044-e5f6262fd33d",
   "metadata": {},
   "source": [
    "### Part 2: Preliminary Network Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdaed46f-ac91-4a9f-bebf-491e7d2559d4",
   "metadata": {},
   "source": [
    "**Network Metrics**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db70708e-1e54-4656-8475-fdf1de7dc6d2",
   "metadata": {},
   "source": [
    "**1. What is the total number of nodes (authors) and links (collaborations) in the network?**\n",
    "\n",
    "Utilizing [G.number_of_nodes()](https://networkx.org/documentation/stable/tutorial.html) and [G.number_of_edges()](https://networkx.org/documentation/stable/tutorial.html) where G is our network graph, we find a total of 14100 nodes (authors) and 49431 links (collaborations) in the network. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e913bc7c-f68e-4797-a32d-489c9535ffe1",
   "metadata": {},
   "source": [
    "**2. Calculate the network's density (the ratio of actual links to the maximum possible number of links). Would you say that the network is sparse? Justify your answer.**\n",
    "\n",
    "When calculating the maximum number of links possible in an undirected graph using the formula L_max=(n*(n-1))/2, where n denotes number of nodes, we find that the maximum number of links for this graph is 99397950. As mentioned above, our graph has a total of 49431 links equal to our network having a density of 0.000497. We would say that the network is sparse as the number of actual links is much smaller than that of L_max, which is to be expected from real-life networks such as this one [4]. \n",
    "\n",
    "[4] Network Science. Albert-László Barabási. 2016. Cambridge United Kingdom: Cambridge University Press."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e993768-9716-4172-a15a-8ed55117da5c",
   "metadata": {},
   "source": [
    "**3. Is the network fully connected (i.e., is there a direct or indirect path between every pair of nodes within the network), or is it disconnected?**\n",
    "\n",
    "Using the length of [list(nx.connected_components(G))](https://networkx.org/documentation/stable/tutorial.html) we see that the graph can be partitioned into multiple connected components meaning the network is considered disconnected. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659435cc-da14-4323-ab50-c72ccf6d6a45",
   "metadata": {},
   "source": [
    "**4. If the network is disconnected, how many connected components does it have? A connected component is defined as a subset of nodes within the network where a path exists between any pair of nodes in that subset.**\n",
    "\n",
    "Using the above method, and given that the network is disconnected, we find that there are 118 connected components. Please see code below as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a58efdf5-8773-43d0-ac1e-46537b88f5d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The network has 118 connected components\n"
     ]
    }
   ],
   "source": [
    "print(\"The network has\", len(list(nx.connected_components(G))), \"connected components\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df26428-4585-4d4d-929b-0cb40d143ab6",
   "metadata": {},
   "source": [
    "**5. How many isolated nodes are there in your network? An isolated node is defined as a node with no connections to any other node in the network.**\n",
    "\n",
    "Using the code in the cell below, we see that there are no isolated nodes in the graph as there are 11 nodes where [G.degree\\[node\\]](https://networkx.org/documentation/stable/tutorial.html) is equal to 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "a087d968-e211-4243-8f36-3026cefbf02d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of isolated nodes: 11\n"
     ]
    }
   ],
   "source": [
    "isolated = 0\n",
    "for i in range(len(combined_df['Author ID'])):\n",
    "    if G.degree(combined_df['Author ID'][i]) == 0:\n",
    "        isolated +=1\n",
    "print(\"Number of isolated nodes:\", isolated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d4f18b-cc14-4e76-90c4-d2a74ed78ad3",
   "metadata": {},
   "source": [
    "**6. Discuss the results above on network density, and connectivity. Are your findings in line with what you expected? Why? (answer in max 150 words)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e904c5-3626-4508-9e42-a9403051d710",
   "metadata": {},
   "source": [
    "We see that the density is quite low, which can be expected from a real-life network, and that within the 14100 authors we initially found, we have seen 118 connected components. As this network represents collaborations on papers between authors, we would expect this level of connectivity and density as research, and the work around writing and publishing papers is quite time-heavy. This number also reflects that the authors included work in different fields of research such as Sociology, Psychology, Economics, and Political Science, have different nationalities, and different places of work. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751659b4-30e8-4a1a-ac2e-8ed24f07deb7",
   "metadata": {},
   "source": [
    "**Degree Analysis:**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5c81beb2-b849-4367-8e01-e0acacd5eb9c",
   "metadata": {},
   "source": [
    "**Compute the average, median, mode, minimum, and maximum degree of the nodes. Perform the same analysis for node strength (weighted degree). What do these metrics tell us about the network? (answer in max 150 words)**\n",
    "\n",
    "Please see the code below for the computation of each descriptive statistic. \n",
    "\n",
    "The network of published work in Computational Social Science and its authors suggests that most authors may collaborate with a few others; this is indicated by the fact that the average weighted degree is higher than the average degree, highlighting significant repeated collaborations among some authors. Conversely, the presence of nodes with a degree of 0 indicates that some authors have not collaborated within the network at all. For both non-weighted and weighted degrees, the large gap between the median and maximum degrees also suggests the existence of a few large and centralized hubs, as well as numerous peripheral ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "6095e89b-8d79-456b-9836-ff05aecf5db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Degrees of nodes:\n",
      "The average degree is: 7.011489361702128\n",
      "The median degree is: 5.0\n",
      "The minimum degree is: 0\n",
      "The maximum degree is: 169\n",
      "The mode degree is: 4\n",
      " \n",
      "Weighted degrees of nodes:\n",
      "The average weighted degree is: 9.173191489361702\n",
      "The median weighted degree is: 6.0\n",
      "The minimum weighted degree is: 0\n",
      "The maximum weighted degree is: 264\n",
      "The mode weighted degree is: 4\n"
     ]
    }
   ],
   "source": [
    "print(\"Degrees of nodes:\")\n",
    "degrees = [G.degree(n) for n in G.nodes()]\n",
    "print(\"The average degree is:\", np.mean(degrees))\n",
    "print(\"The median degree is:\", np.median(degrees))\n",
    "print(\"The minimum degree is:\", min(degrees))\n",
    "print(\"The maximum degree is:\", max(degrees))\n",
    "print(\"The mode degree is:\", mode(degrees))\n",
    "print(\" \")\n",
    "print(\"Weighted degrees of nodes:\")\n",
    "weighted_degrees = [G.degree(n, weight='weight') for n in G.nodes()]\n",
    "print(\"The average weighted degree is:\", np.mean(weighted_degrees))\n",
    "print(\"The median weighted degree is:\", np.median(weighted_degrees))\n",
    "print(\"The minimum weighted degree is:\", min(weighted_degrees))\n",
    "print(\"The maximum weighted degree is:\", max(weighted_degrees))\n",
    "print(\"The mode weighted degree is:\", mode(weighted_degrees))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f35954f3",
   "metadata": {},
   "source": [
    "**Top Authors:**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c85e9a8e",
   "metadata": {},
   "source": [
    "**1. Identify the top 5 authors by degree. What role do these nodes play in the network?**\n",
    "\n",
    "The top 5 authors by degree is: \n",
    "\n",
    "- **Qin Li with degree = 169** from Central South University in China \n",
    "- **Mariano Sigman with degree = 155** from Universidad Torcuato Di Tella in Argentina\n",
    "- **Stephan Lewandowsky with degree = 150** from University of Bristol, University of Western Australia, and University of Potsdam\n",
    "- **Denny Borsboom with degree = 140** from University of Amsterdam\n",
    "- **Dan Jurafsky with degree = 129** from Stanford University\n",
    "\n",
    "These are authors who have collaborated with many other authors on papers included in the data we are working with. This could be due to them having special expertise within a certain area detrimental to other researchers, or that they have supervised other researchers in their projects. They could be the nodes who connects clusters of researchers that otherwise would not have been connected, which would show up in the number of connected components. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "55abe0e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes with highest degrees: [('https://openalex.org/A5075080019', 169), ('https://openalex.org/A5005493160', 155), ('https://openalex.org/A5017914184', 150), ('https://openalex.org/A5029100305', 140), ('https://openalex.org/A5087088138', 129)]\n"
     ]
    }
   ],
   "source": [
    "degrees = G.degree()\n",
    "top_5_nodes = sorted(degrees, key=lambda x: x[1], reverse=True)[:5]\n",
    "top_5_nodes = [(node, degree) for node, degree in top_5_nodes]\n",
    "print(\"Nodes with highest degrees:\", top_5_nodes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1e5fe699",
   "metadata": {},
   "source": [
    "**2. Research these authors online. What areas do they specialize in? Do you think that their work aligns with the themes of Computational Social Science? If not, what could be possible reasons? (answer in max 150 words)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23e2f19",
   "metadata": {},
   "source": [
    "[Qin Li](https://scholar.google.com/citations?user=6-u5zzMAAAAJ&hl=en) is a PhD, School of Energy Science and Engineering. [Mariano Sigman](https://www.marianosigman.org/en/) is a neuroscientist and author. [Stephan Lewandowsky](https://en.wikipedia.org/wiki/Stephan_Lewandowsky) is a psychologist who is currently head of Cognitive Psychology at University of Bristol, and whos work was originally on \"computer simulations of people's decision-making processes\". [Denny Borsboom](https://dennyborsboom.com) is also a psychologist and psychometrician who use network theory for his work. [Dan Jurafsky](https://web.stanford.edu/~jurafsky/) is a professor in linguistics and computer science, who study NLP and its application in social and cognitive sciences. \n",
    "\n",
    "As we see, these authors are not all within fields of research that are exclusivly within the themes of Computational Social Science. As mentioned in the previous question, their degrees (within the network) could possibly be explained by their expertise within their respective fields which researchers whos work align with the themes of Computational Social Science could benefit from. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
