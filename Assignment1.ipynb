{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a93c99e0-22aa-4247-9d61-4f162eb930d1",
   "metadata": {},
   "source": [
    "## Assignment 1\n",
    "### Group 14 \n",
    "\n",
    "> Github repository: https://github.com/LivDreyer/CSS24.git\n",
    "\n",
    "> History of commits:\n",
    "\n",
    "> Contribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "88b9e9fb-277c-4d83-9a3b-75b6664ee679",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importations \n",
    "import requests \n",
    "from bs4 import BeautifulSoup as BS\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import ast\n",
    "import networkx as nx\n",
    "import matplotlib\n",
    "import requests\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2049e25-96ed-4ce9-80d5-d058adfd7349",
   "metadata": {},
   "source": [
    "## Part 1: Web-scrabing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d3b36f-6ce6-4fdf-b686-c8082b8fefde",
   "metadata": {},
   "source": [
    "In this part, we have included keynote speakers, chairs, authors of parallel talks, authors of posters, the names of researchers from the programme committee of the conference, and the organizers of tutorials. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1228110-abf3-4875-b69f-dbaef8928520",
   "metadata": {},
   "source": [
    "#### Names from the program: Plenary Speakers, Keynote Speakers, Authors of Parallel Talks, Authors of Posters, and Chairs\n",
    "#### Link: https://ic2s2-2023.org/program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8f67921-69f9-4beb-b725-d46f9769c6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get html to py\n",
    "\n",
    "link = \"https://ic2s2-2023.org/program\"\n",
    "r = requests.get(link)\n",
    "soup = BS(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b3f21a3-8e2e-41f3-bb18-cb74fe0eefa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plenary, parallel talks, posters speaker\n",
    "# Name of final list: ppp_names\n",
    "\n",
    "#find programs from each day \n",
    "plen = soup.find_all(\"ul\", {\"class\":\"nav_list\"})\n",
    "\n",
    "#find all names from the day \n",
    "names_plen = []\n",
    "names_plen_text = []\n",
    "for i in range(len(plen)): \n",
    "    names_plen.append(plen[i].find_all(\"i\"))\n",
    "\n",
    "for i in range(len(names_plen)):\n",
    "    for j in range(len(names_plen[i])):\n",
    "        names_plen_text.append(names_plen[i][j].text)\n",
    "\n",
    "plen_names_lst = [name for names in names_plen_text for name in names.split(', ')]\n",
    "\n",
    "ppp_names = []\n",
    "for i in range(len(plen_names_lst)): \n",
    "    ppp_names.append(plen_names_lst[i].title())\n",
    "\n",
    "\n",
    "for name in ppp_names:\n",
    "    if '\\u201a' in name:\n",
    "        ppp_names.remove(name)\n",
    "\n",
    "#print(ppp_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be2eb25d-3e1e-4b81-8e72-920d53c44324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keynote speakers \n",
    "# Name of final list: key_names\n",
    "\n",
    "table = soup.find_all(\"table\", {\"class\": \"tutorials\"})\n",
    "table = table[0].find_all(\"td\")\n",
    "\n",
    "table_text = []\n",
    "for i in range(len(table)):\n",
    "    txt = table[i].text\n",
    "    if \"Keynote\" in txt: \n",
    "        table_text.append(txt)\n",
    "\n",
    "key_names = []\n",
    "for i in table_text: \n",
    "    key_names.append(i[10:])\n",
    "\n",
    "\n",
    "#print(key_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "308d7707-ef51-4ae9-abaa-c8c6a691694a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chairs\n",
    "# Name of final list: chair_names\n",
    "\n",
    "\n",
    "chair = soup.find_all(\"div\", {\"class\":\"col-9 col-12-medium\"})\n",
    "\n",
    "#find chairs\n",
    "names_chairs = []\n",
    "names_chairs_text = []\n",
    "for i in range(len(chair)): \n",
    "    names_chairs.append(chair[i].find_all(\"h2\"))\n",
    "\n",
    "for i in range(len(names_chairs)):\n",
    "    for j in range(len(names_chairs[i])):\n",
    "        names_chairs_text.append(names_chairs[i][j].text)\n",
    "\n",
    "\n",
    "chr = []\n",
    "\n",
    "for i in names_chairs_text:\n",
    "    if \"Chair\" in i: \n",
    "        chr.append(i)\n",
    "\n",
    "chair_names = []\n",
    "for i in chr: \n",
    "    idx = i.find(\"Chair:\")\n",
    "    chair_names.append(i[idx + len(\"Chair:\"):].strip())\n",
    "    \n",
    "    \n",
    "#print(chair_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bae853-870f-4a0d-8715-f304c1e3c18f",
   "metadata": {},
   "source": [
    "#### Names from the programme committee \n",
    "#### Link: https://ic2s2-2023.org/program_committee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dae3a5b1-2d42-482d-8598-486a9752d7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "link_pc = \"https://ic2s2-2023.org/program_committee\"\n",
    "r_pc = requests.get(link_pc)\n",
    "soup_pc = BS(r_pc.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "860ff4b1-b313-4481-83c4-f2fd70836ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Programme Committee \n",
    "# Name of final list: pcom_names \n",
    "\n",
    "pc = soup_pc.find_all(\"article\", {\"class\":\"box post\"})\n",
    "\n",
    "#find all names from the day \n",
    "pc_names = []\n",
    "pc_text = []\n",
    "for i in range(len(pc)): \n",
    "    pc_names.append(pc[i].find_all(\"li\"))\n",
    "\n",
    "for i in range(len(pc_names)):\n",
    "    for j in range(len(pc_names[i])):\n",
    "        pc_text.append(pc_names[i][j].text)\n",
    "\n",
    "\n",
    "pcom_names = []\n",
    "for i in range(len(pc_text)):\n",
    "    lst = pc_text[i].split(\" (\")\n",
    "    pcom_names.append(lst[0])\n",
    "\n",
    "#print(pcom_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfbc99c-2512-43c1-b134-cb3130f252d2",
   "metadata": {},
   "source": [
    "#### Names from the Tutorials\n",
    "#### Link: https://ic2s2-2023.org/tutorials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87229e41-7f5c-45ba-aa9d-16f75c6f78ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "link_t = \"https://ic2s2-2023.org/tutorials\"\n",
    "r_t = requests.get(link_t)\n",
    "soup_t = BS(r_t.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8ed48a0-0b43-4ed1-a975-27fdef56cd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tutorials \n",
    "# Name of final list: tut_names_f\n",
    "\n",
    "tut = soup_t.find_all(\"div\", {\"class\": \"col-5 col-12-medium\"})\n",
    "\n",
    "#find all names from the day \n",
    "tut_names = []\n",
    "tut_text = []\n",
    "for i in range(len(tut)): \n",
    "    tut_names.append(tut[i].find_all(\"li\"))\n",
    "\n",
    "for i in range(len(tut_names)):\n",
    "    for j in range(len(tut_names[i])):\n",
    "        tut_text.append(tut_names[i][j].text)\n",
    "\n",
    "#print(tut_text)\n",
    "\n",
    "tut_names_f = []\n",
    "for i in range(len(tut_text)):\n",
    "    lst = tut_text[i].split(\",\")\n",
    "    tut_names_f.append(lst[0])\n",
    "\n",
    "#print(tut_names_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5faf36ea-c00d-4b11-94e3-afb5cea807a1",
   "metadata": {},
   "source": [
    "### Collect all names in one final string and process "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "828578ac-19c3-4a6b-bd2a-3c599d8cac81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of names including optional exercises:  1620\n"
     ]
    }
   ],
   "source": [
    "# Collect all names in one list of strings \n",
    "\n",
    "all_names = chair_names + key_names + ppp_names + pcom_names + tut_names_f\n",
    "#print(all_names)\n",
    "\n",
    "#Capitalize every name \n",
    "all_names_final = []\n",
    "for i in range(len(all_names)): \n",
    "    all_names_final.append(all_names[i].title())\n",
    "\n",
    "all_names_final = list(set(all_names_final))\n",
    "\n",
    "\n",
    "#Pandas DF \n",
    "names_DF = pd.DataFrame(all_names_final, columns=['Researcher Name'])\n",
    "names_DF = names_DF.sort_values(by='Researcher Name')\n",
    "\n",
    "#Split names into first name and last name, dropping middle names in all formats: \"John Name Doe\" or \"John N. Doe\"\n",
    "names_DF['first_name'] = names_DF['Researcher Name'].str.split().str[0]\n",
    "names_DF['last_name'] = names_DF['Researcher Name'].str.split().str[-1]\n",
    "\n",
    "#Remove duplicates of names such as \"John Name Doe\" or \"John N. Doe\"\n",
    "names_DF.drop_duplicates(subset=['first_name', 'last_name'], inplace=True)\n",
    "\n",
    "#Save first column (names) to file\n",
    "names_DF['Researcher Name'].to_csv('/Users/livdreyerjohansen/Desktop/CSS/Names_DF_Ass1.csv', index=False)\n",
    "\n",
    "#Find number of researchers\n",
    "\n",
    "print(\"Number of names including optional exercises: \",len(names_DF))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fb5ce2-3445-4400-8b8b-055b30d5d64f",
   "metadata": {},
   "source": [
    "### How many unique researchers did you get? \n",
    "\n",
    "As seen above, we got 1620 unique names of researchers when including the optional exercises. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7c4e98-f138-497c-883b-0044e50c5640",
   "metadata": {},
   "source": [
    "### Explain the process you followed to web-scrape the page. Which choices did you make to accurately retreive as many names as possible? Which strategies did you use to assess the quality of your final list? Explain your reasoning and your choices (answer in max 150 words)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a46001-f5fb-4cf5-9f7a-638e64874c3e",
   "metadata": {},
   "source": [
    "When initially inspecting the webpage to understand how it was structured, and where we should look, we relied on visual cues signaling what html-code corresponded to the data we wanted. Working with retrieving names of researchers from the program, we structured the retrieval in three parts, as we recognized different patterns for each type of name. We repeated this process for the optional exercise. For each part we used the [BeautifulSoup Python package](https://pypi.org/project/beautifulsoup4/)'s .[find_all](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#find-all) method to extract the wanted names, and lastly combined every list of names to one final list. For the final list, we capitalized each name, and then used the set() method to remove apparent duplicates. We then created a Pandas Dataframe, where we split every name into first and last name excluding the middle name. It was then possible to remove rows with identical first and last name, letting us create a csv.file with unique “Researcher Names”."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f4333c-00da-4ef8-9d02-1b639c909964",
   "metadata": {},
   "source": [
    "## Part 2: Ready Made vs Custom Made Data\n",
    "\n",
    "*Exercise: Ready made data vs Custom made data In this exercise, I want to make sure you have understood they key points of my lecture and the reading.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01dbd011-835b-437f-add5-4e94667adfda",
   "metadata": {},
   "source": [
    "**What are pros and cons of the custom-made data used in Centola's experiment (the first study presented in the lecture) and the ready-made data used in Nicolaides's study (the second study presented in the lecture)? You can support your arguments based on the content of the lecture and the information you read in Chapter 2.3 of the book (answer in max 150 words).**\n",
    "\n",
    "Centola’s experiment [1] allows the investigation of diffusion processes with no influence from confounding variables such as advertising or social reinforcement from outside of the experiment. Though generally desired it’s hard to apply the results directly on real-word settings. Despite a smaller sample size, Centola could repeat the experiment to reproduce tendencies of spread of information. A problem of volunteer bias could however possibly arise from how he recruited participants.\n",
    "In Nicolaides’s study [2] the large sample size and the non-reactivity of users constitute an advantage when investigating contagion as it can mirror real-world conditions. However, as mentioned in “Bit by bit” 2.3.3 [3], non-reactivity does not ensure that the data directly reflects behavior. Users of the fitness-app is not forced to participate even though they have connections on the app, and to paraphrase from the book: “It’s not that I am not exercising, I am just not posting it on thefitness- app”. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51fea8d0-8ec6-4db8-a2e5-19a4c5a22d01",
   "metadata": {},
   "source": [
    "**How do you think these differences can influence the interpretation of the results in each study? (answer in max 150 words)**\n",
    "\n",
    "Centola’s findings are based on an artificial environment that find significant results mapping the “essence” of behavioral contagion, however, the reactivity of participants could influence the interpretation of the results. Additionally, as mentioned, the participants were recruited from health-sites which could promote the behavior Centola finds considering his experiment is also concerned with health behavior. \n",
    "\n",
    "Depending on how Nicolaides performs the linear regression to examine how one user’s exercise might influence their connections’ exercise on the fitness-tracker app, it might not mirror the actual contagion happening. If people are active users who follow their friends, but do not post their exercise themselves, the results of the linear regression may reflect this more than the actual influence users have on their connections.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f529d25e-7185-433f-9791-e582c2d7bdbe",
   "metadata": {},
   "source": [
    "[1] Damon Centola ,The Spread of Behavior in an Online Social Network Experiment.Science329,1194-1197(2010).DOI:10.1126/science.1185231\n",
    "\n",
    "[2] Aral S, Nicolaides C. Exercise contagion in a global social network. Nat Commun. 2017 Apr 18;8:14753. doi: 10.1038/ncomms14753.\n",
    "\n",
    "[3] Salganik, Matthew J. 2017. Bit by Bit: Social Research in the Digital Age. Princeton, NJ: Princeton University Press. Open review edition."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b945e157-2ddc-4340-9888-80f507371ed7",
   "metadata": {},
   "source": [
    "## Part 3: Gathering Research Articles using the OpenAlex API"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "016789ff",
   "metadata": {},
   "source": [
    "### Step 1: Retrieve data. \n",
    "\n",
    "**Starting with the authors you identified in Week 2, Exercise 2, use the OpenAlex API works endpoint to fetch the research articles they have authored.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87909c32",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "985eae0c",
   "metadata": {},
   "source": [
    "## Part 4: The Network of Computational Social Scientists"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2e2674-1508-4714-bbb4-594fa096b652",
   "metadata": {},
   "source": [
    "### Part 1: Network Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7b458b-d498-4ec4-8b1f-85211236f87e",
   "metadata": {},
   "source": [
    "**Weighted Edgelist Creation:** Start with your dataframe of papers. Construct a weighted edgelist where each list element is a tuple containing three elements: the author ids of two collaborating authors and the total number of papers they've co-authored. Ensure each author pair is listed only once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d51ed386-b448-41a3-aba1-0ad64aca1976",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load dataframe \n",
    "\n",
    "df = pd.read_csv(\"/Users/livdreyerjohansen/Desktop/CSS/final_papers_dataset.csv\", usecols = [\"id\",\"author_ids\"])\n",
    "\n",
    "df['author_ids'] = df['author_ids'].apply(ast.literal_eval) # convert string to list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ddd7ab54-1e14-4dc4-95c2-4ff9de2755a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create List of Collaborations between authors\n",
    "\n",
    "collabs = []\n",
    "for work_idx in range(len(df[\"id\"])):\n",
    "    work = df[\"id\"][work_idx]\n",
    "    collabs.extend([(a, b) for idx, a in enumerate(df[\"author_ids\"][work_idx]) for b in df[\"author_ids\"][work_idx][idx + 1:]])\n",
    "    if work_idx == 100:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a14a58b-572d-44d2-a51e-599ab3a12fa8",
   "metadata": {},
   "source": [
    "**Graph Construction:**\n",
    "\n",
    "- Use NetworkX to create an undirected Graph.\n",
    "- Employ the add_weighted_edges_from function to populate the graph with the weighted edgelist from step 1, creating a weighted, undirected graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1174767-6fed-41e1-9492-3ac7a71bccc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count the number of occurences of the Collaborations\n",
    "\n",
    "authors_collabs = []\n",
    "for authors in collabs:\n",
    "    count = 0\n",
    "    for j in range(len(collabs)):\n",
    "        if authors == collabs[j]:\n",
    "            count += 1\n",
    "    authors_collabs.append((authors[0],authors[1],count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8cd20fa5-e2d3-4c33-a196-4daf91d67275",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create graph G\n",
    "\n",
    "G = nx.Graph()\n",
    "G.add_weighted_edges_from(authors_collabs)\n",
    "G.edges(data=True)\n",
    "\n",
    "G.nodes['https://openalex.org/A5075442335']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "81d44d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get unique authors in the collaborations\n",
    "authors = []\n",
    "for i in range(len(collabs)):\n",
    "    if collabs[i][0] not in authors:\n",
    "        authors.append(collabs[i][0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "81d8060a-180b-4bd3-abe9-60d21e9e5454",
   "metadata": {},
   "source": [
    "**Node Attributes:**\n",
    "\n",
    "- For each node, add attributes for the author's *display name*, *country*, *citation count*, and the *year of their first publication* in Computational Social Science. The *display name* and *country* can be retrieved from your *authors* dataset. The *year of their first publication* and the *citation count* can be retrieved from the *papers* dataset.\n",
    "- Save the network as a JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2d139013-4e3f-4080-b08b-b5a883556472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://openalex.org/A5075442335\n"
     ]
    }
   ],
   "source": [
    "G = nx.Graph()\n",
    "G.add_weighted_edges_from(authors_collabs)\n",
    "G.edges(data=True)\n",
    "\n",
    "for i in range(1):\n",
    "    BASE_URL = \"https://api.openalex.org/authors\"\n",
    "    SEARCH = f\"/{authors[i]}\"\n",
    "    SELECT = \"?select=id,display_name,works_api_url,cited_by_count,last_known_institutions\"\n",
    "\n",
    "    req = requests.get(BASE_URL + SEARCH + SELECT)\n",
    "\n",
    "    result = req.json()\n",
    "    country = result['last_known_institutions'][0]['country_code']\n",
    "    name = result['display_name']\n",
    "    cites = result['cited_by_count']\n",
    "    id = result['id']\n",
    "    req2 = requests.get(result[\"works_api_url\"])\n",
    "\n",
    "    authors_work = req2.json()\n",
    "    date = datetime.strptime(\"3000-01-01\", \"%Y-%m-%d\")\n",
    "    # for j in range(len(authors_work['results'])):\n",
    "    for j in range(len(authors_work['results'])):\n",
    "        field = authors_work['results'][j]['primary_topic']['field']['display_name']\n",
    "        if field == \"Sociology\" or \"Psychology\" or \"Economics\" or \"Political Science\":\n",
    "            date_string = authors_work['results'][j]['created_date']\n",
    "            if date > datetime.strptime(date_string, \"%Y-%m-%d\"):\n",
    "                date = datetime.strptime(date_string, \"%Y-%m-%d\")\n",
    "                \n",
    "    year_firstpub = date.date().year\n",
    "    author_attribute = {'display_name': name, 'country': country, 'citation_count': cites, 'first_publication' : year_firstpub}\n",
    "    nx.set_node_attributes(G, {id : author_attribute})\n",
    "    print(id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b5933d9-266c-43e6-8397-d93008bd1a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c483d841-fcc0-410d-b044-e5f6262fd33d",
   "metadata": {},
   "source": [
    "### Part 2: Preliminary Network Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdaed46f-ac91-4a9f-bebf-491e7d2559d4",
   "metadata": {},
   "source": [
    "**Network Metrics**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db70708e-1e54-4656-8475-fdf1de7dc6d2",
   "metadata": {},
   "source": [
    "**1. What is the total number of nodes (authors) and links (collaborations) in the network?**\n",
    "\n",
    "Utilizing [G.number_of_nodes()](https://networkx.org/documentation/stable/tutorial.html) and [G.number_of_edges()](https://networkx.org/documentation/stable/tutorial.html) where G is our network graph, we find a total of 332 nodes (authors) and 6291 links (collaborations) in the network. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e913bc7c-f68e-4797-a32d-489c9535ffe1",
   "metadata": {},
   "source": [
    "**2. Calculate the network's density (the ratio of actual links to the maximum possible number of links). Would you say that the network is sparse? Justify your answer.**\n",
    "\n",
    "When calculating the maximum number of links possible in an undirected graph using the formula (n*(n-1))/2, where n denotes number of nodes, we find that the maximum number of links for this graph is 54946. As mentioned above, our graph has a total of 6291 links equal to our network having a density of 0.114. **Would you say that the network is sparse? Justify your answer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e993768-9716-4172-a15a-8ed55117da5c",
   "metadata": {},
   "source": [
    "**3. Is the network fully connected (i.e., is there a direct or indirect path between every pair of nodes within the network), or is it disconnected?**\n",
    "\n",
    "Using the length of [list(nx.connected_components(G))](https://networkx.org/documentation/stable/tutorial.html) we see that the graph can be partitioned into multiple connected components meaning the network is considered disconnected. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659435cc-da14-4323-ab50-c72ccf6d6a45",
   "metadata": {},
   "source": [
    "**4. If the network is disconnected, how many connected components does it have? A connected component is defined as a subset of nodes within the network where a path exists between any pair of nodes in that subset.**\n",
    "\n",
    "Using the above method, and given that the network is disconnected, we find that there are 4 connected components. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df26428-4585-4d4d-929b-0cb40d143ab6",
   "metadata": {},
   "source": [
    "**5. How many isolated nodes are there in your network? An isolated node is defined as a node with no connections to any other node in the network.**\n",
    "\n",
    "Using the code in the cell below, we see that there are no isolated nodes in the graph as there are no nodes where [G.degree\\[node\\]](https://networkx.org/documentation/stable/tutorial.html) is equal to 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a087d968-e211-4243-8f36-3026cefbf02d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of isolated nodes: 0\n"
     ]
    }
   ],
   "source": [
    "isolated = 0\n",
    "for i in range(len(authors)):\n",
    "    if G.degree(authors[i]) == 0:\n",
    "        isolated +=1\n",
    "print(\"Number of isolated nodes:\", isolated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d4f18b-cc14-4e76-90c4-d2a74ed78ad3",
   "metadata": {},
   "source": [
    "**6. Discuss the results above on network density, and connectivity. Are your findings in line with what you expected? Why? (answer in max 150 words)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e904c5-3626-4508-9e42-a9403051d710",
   "metadata": {},
   "source": [
    "Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751659b4-30e8-4a1a-ac2e-8ed24f07deb7",
   "metadata": {},
   "source": [
    "**Degree Analysis:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c81beb2-b849-4367-8e01-e0acacd5eb9c",
   "metadata": {},
   "source": [
    "**Compute the average, median, mode, minimum, and maximum degree of the nodes. Perform the same analysis for node strength (weighted degree). What do these metrics tell us about the network? (answer in max 150 words)**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f35954f3",
   "metadata": {},
   "source": [
    "**Top Authors:**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c85e9a8e",
   "metadata": {},
   "source": [
    "**1. Identify the top 5 authors by degree. What role do these node play in the network?**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1e5fe699",
   "metadata": {},
   "source": [
    "**2. Research these authors online. What areas do they specialize in? Do you think that their work aligns with the themes of Computational Social Science? If not, what could be possible reasons? (answer in max 150 words)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23e2f19",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
