{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a93c99e0-22aa-4247-9d61-4f162eb930d1",
   "metadata": {},
   "source": [
    "## Assignment 1\n",
    "### Group 14 \n",
    "\n",
    "> Github repository: https://github.com/LivDreyer/CSS24.git\n",
    "\n",
    "> History of commits:\n",
    "\n",
    "> Contribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "88b9e9fb-277c-4d83-9a3b-75b6664ee679",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importations \n",
    "import requests \n",
    "from bs4 import BeautifulSoup as BS\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2049e25-96ed-4ce9-80d5-d058adfd7349",
   "metadata": {},
   "source": [
    "## Part 1: Web-scrabing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d3b36f-6ce6-4fdf-b686-c8082b8fefde",
   "metadata": {},
   "source": [
    "In this part, we have included keynote speakers, chairs, authors of parallel talks, authors of posters, the names of researchers from the programme committee of the conference, and the organizers of tutorials. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1228110-abf3-4875-b69f-dbaef8928520",
   "metadata": {},
   "source": [
    "#### Names from the program: Plenary Speakers, Keynote Speakers, Authors of Parallel Talks, Authors of Posters, and Chairs\n",
    "#### Link: https://ic2s2-2023.org/program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e8f67921-69f9-4beb-b725-d46f9769c6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get html to py\n",
    "\n",
    "link = \"https://ic2s2-2023.org/program\"\n",
    "r = requests.get(link)\n",
    "soup = BS(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5b3f21a3-8e2e-41f3-bb18-cb74fe0eefa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plenary, parallel talks, posters speaker\n",
    "# Name of final list: ppp_names\n",
    "\n",
    "#find programs from each day \n",
    "plen = soup.find_all(\"ul\", {\"class\":\"nav_list\"})\n",
    "\n",
    "#find all names from the day \n",
    "names_plen = []\n",
    "names_plen_text = []\n",
    "for i in range(len(plen)): \n",
    "    names_plen.append(plen[i].find_all(\"i\"))\n",
    "\n",
    "for i in range(len(names_plen)):\n",
    "    for j in range(len(names_plen[i])):\n",
    "        names_plen_text.append(names_plen[i][j].text)\n",
    "\n",
    "plen_names_lst = [name for names in names_plen_text for name in names.split(', ')]\n",
    "\n",
    "ppp_names = []\n",
    "for i in range(len(plen_names_lst)): \n",
    "    ppp_names.append(plen_names_lst[i].title())\n",
    "\n",
    "\n",
    "for name in ppp_names:\n",
    "    if '\\u201a' in name:\n",
    "        ppp_names.remove(name)\n",
    "\n",
    "#print(ppp_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "be2eb25d-3e1e-4b81-8e72-920d53c44324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keynote speakers \n",
    "# Name of final list: key_names\n",
    "\n",
    "table = soup.find_all(\"table\", {\"class\": \"tutorials\"})\n",
    "table = table[0].find_all(\"td\")\n",
    "\n",
    "table_text = []\n",
    "for i in range(len(table)):\n",
    "    txt = table[i].text\n",
    "    if \"Keynote\" in txt: \n",
    "        table_text.append(txt)\n",
    "\n",
    "key_names = []\n",
    "for i in table_text: \n",
    "    key_names.append(i[10:])\n",
    "\n",
    "\n",
    "#print(key_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "308d7707-ef51-4ae9-abaa-c8c6a691694a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chairs\n",
    "# Name of final list: chair_names\n",
    "\n",
    "\n",
    "chair = soup.find_all(\"div\", {\"class\":\"col-9 col-12-medium\"})\n",
    "\n",
    "#find chairs\n",
    "names_chairs = []\n",
    "names_chairs_text = []\n",
    "for i in range(len(chair)): \n",
    "    names_chairs.append(chair[i].find_all(\"h2\"))\n",
    "\n",
    "for i in range(len(names_chairs)):\n",
    "    for j in range(len(names_chairs[i])):\n",
    "        names_chairs_text.append(names_chairs[i][j].text)\n",
    "\n",
    "\n",
    "chr = []\n",
    "\n",
    "for i in names_chairs_text:\n",
    "    if \"Chair\" in i: \n",
    "        chr.append(i)\n",
    "\n",
    "chair_names = []\n",
    "for i in chr: \n",
    "    idx = i.find(\"Chair:\")\n",
    "    chair_names.append(i[idx + len(\"Chair:\"):].strip())\n",
    "    \n",
    "    \n",
    "#print(chair_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bae853-870f-4a0d-8715-f304c1e3c18f",
   "metadata": {},
   "source": [
    "#### Names from the programme committee \n",
    "#### Link: https://ic2s2-2023.org/program_committee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "dae3a5b1-2d42-482d-8598-486a9752d7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "link_pc = \"https://ic2s2-2023.org/program_committee\"\n",
    "r_pc = requests.get(link_pc)\n",
    "soup_pc = BS(r_pc.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "860ff4b1-b313-4481-83c4-f2fd70836ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Programme Committee \n",
    "# Name of final list: pcom_names \n",
    "\n",
    "pc = soup_pc.find_all(\"article\", {\"class\":\"box post\"})\n",
    "\n",
    "#find all names from the day \n",
    "pc_names = []\n",
    "pc_text = []\n",
    "for i in range(len(pc)): \n",
    "    pc_names.append(pc[i].find_all(\"li\"))\n",
    "\n",
    "for i in range(len(pc_names)):\n",
    "    for j in range(len(pc_names[i])):\n",
    "        pc_text.append(pc_names[i][j].text)\n",
    "\n",
    "\n",
    "pcom_names = []\n",
    "for i in range(len(pc_text)):\n",
    "    lst = pc_text[i].split(\" (\")\n",
    "    pcom_names.append(lst[0])\n",
    "\n",
    "#print(pcom_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfbc99c-2512-43c1-b134-cb3130f252d2",
   "metadata": {},
   "source": [
    "#### Names from the Tutorials\n",
    "#### Link: https://ic2s2-2023.org/tutorials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "87229e41-7f5c-45ba-aa9d-16f75c6f78ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "link_t = \"https://ic2s2-2023.org/tutorials\"\n",
    "r_t = requests.get(link_t)\n",
    "soup_t = BS(r_t.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e8ed48a0-0b43-4ed1-a975-27fdef56cd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tutorials \n",
    "# Name of final list: tut_names_f\n",
    "\n",
    "tut = soup_t.find_all(\"div\", {\"class\": \"col-5 col-12-medium\"})\n",
    "\n",
    "#find all names from the day \n",
    "tut_names = []\n",
    "tut_text = []\n",
    "for i in range(len(tut)): \n",
    "    tut_names.append(tut[i].find_all(\"li\"))\n",
    "\n",
    "for i in range(len(tut_names)):\n",
    "    for j in range(len(tut_names[i])):\n",
    "        tut_text.append(tut_names[i][j].text)\n",
    "\n",
    "#print(tut_text)\n",
    "\n",
    "tut_names_f = []\n",
    "for i in range(len(tut_text)):\n",
    "    lst = tut_text[i].split(\",\")\n",
    "    tut_names_f.append(lst[0])\n",
    "\n",
    "#print(tut_names_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5faf36ea-c00d-4b11-94e3-afb5cea807a1",
   "metadata": {},
   "source": [
    "### Collect all names in one final string and process "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "828578ac-19c3-4a6b-bd2a-3c599d8cac81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of names including optional exercises:  1620\n"
     ]
    }
   ],
   "source": [
    "# Collect all names in one list of strings \n",
    "\n",
    "all_names = chair_names + key_names + ppp_names + pcom_names + tut_names_f\n",
    "#print(all_names)\n",
    "\n",
    "#Capitalize every name \n",
    "all_names_final = []\n",
    "for i in range(len(all_names)): \n",
    "    all_names_final.append(all_names[i].title())\n",
    "\n",
    "all_names_final = list(set(all_names_final))\n",
    "\n",
    "\n",
    "#Pandas DF \n",
    "names_DF = pd.DataFrame(all_names_final, columns=['Researcher Name'])\n",
    "names_DF = names_DF.sort_values(by='Researcher Name')\n",
    "\n",
    "#Split names into first name and last name, dropping middle names in all formats: \"John Name Doe\" or \"John N. Doe\"\n",
    "names_DF['first_name'] = names_DF['Researcher Name'].str.split().str[0]\n",
    "names_DF['last_name'] = names_DF['Researcher Name'].str.split().str[-1]\n",
    "\n",
    "#Remove duplicates of names such as \"John Name Doe\" or \"John N. Doe\"\n",
    "names_DF.drop_duplicates(subset=['first_name', 'last_name'], inplace=True)\n",
    "\n",
    "#Save first column (names) to file\n",
    "names_DF['Researcher Name'].to_csv('/Users/livdreyerjohansen/Desktop/CSS/Names_DF_Ass1.csv', index=False)\n",
    "\n",
    "#Find number of researchers\n",
    "\n",
    "print(\"Number of names including optional exercises: \",len(names_DF))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fb5ce2-3445-4400-8b8b-055b30d5d64f",
   "metadata": {},
   "source": [
    "### How many unique researchers did you get? \n",
    "\n",
    "As seen above, we got 1620 unique names of researchers when including the optional exercises. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7c4e98-f138-497c-883b-0044e50c5640",
   "metadata": {},
   "source": [
    "### Explain the process you followed to web-scrape the page. Which choices did you make to accurately retreive as many names as possible? Which strategies did you use to assess the quality of your final list? Explain your reasoning and your choices (answer in max 150 words)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a46001-f5fb-4cf5-9f7a-638e64874c3e",
   "metadata": {},
   "source": [
    "When initially inspecting the webpage to understand how it was structured, and where we should look, we relied on visual cues signaling what html-code corresponded to the data we wanted. Working with retrieving names of researchers from the program, we structured the retrieval in three parts, as we recognized different patterns for each type of name. We repeated this process for the optional exercise. For each part we used the BeautifulSoup’s .find_all() method to extract the wanted names, and lastly combined every list of names to one final list. For the final list, we capitalized each name, and then used the set() method to remove apparent duplicates. We then created a Pandas Dataframe, where we split every name into first and last name excluding the middle name. It was then possible to remove rows with identical first and last name, letting us create a csv.file with unique “Researcher Names”."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f4333c-00da-4ef8-9d02-1b639c909964",
   "metadata": {},
   "source": [
    "## Part 2: Ready Made vs Custom Made Data\n",
    "\n",
    "*Exercise: Ready made data vs Custom made data In this exercise, I want to make sure you have understood they key points of my lecture and the reading.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01dbd011-835b-437f-add5-4e94667adfda",
   "metadata": {},
   "source": [
    "**What are pros and cons of the custom-made data used in Centola's experiment (the first study presented in the lecture) and the ready-made data used in Nicolaides's study (the second study presented in the lecture)? You can support your arguments based on the content of the lecture and the information you read in Chapter 2.3 of the book (answer in max 150 words).**\n",
    "\n",
    "Centola’s experiment allows the investigation of diffusion processes with no influence from confounding variables such as advertising or social reinforcement from outside of the experiment. Though generally desired it’s hard to apply the results directly on real-word settings. Despite a smaller sample size, Centola could repeat the experiment to reproduce tendencies of spread of information. A problem of volunteer bias could however possibly arise from how he recruited participants.\r\n",
    "In Nicolaides’s study the large sample size and the non-reactivity of users constitute an advantage when investigating contagion as it can mirror real-world conditions. However, as mentioned in “Bit by bit” 2.3.3, non-reactivity does not ensure that the data directly reflects behavior. Users of the fitness-app is not forced to participate even though they have connections on the app, and to paraphrase from the book: “It’s not that I am not exercising, I am just not posting it on thefitness- app”. \r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51fea8d0-8ec6-4db8-a2e5-19a4c5a22d01",
   "metadata": {},
   "source": [
    "**How do you think these differences can influence the interpretation of the results in each study? (answer in max 150 words)**\n",
    "\n",
    "Centola’s findings are based on an artificial environment that find significant results mapping the “essence” of behavioral contagion, however, the reactivity of participants could influence the interpretation of the results. Additionally, as mentioned, the participants were recruited from health-sites which could promote the behavior Centola finds considering his experiment is also concerned with health behavior. \r\n",
    "\r\n",
    "Depending on how Nicolaides performs the linear regression to examine how one user’s exercise might influence their connections’ exercise on the fitness-tracker app, it might not mirror the actual contagion happening. If people are active users who follow their friends, but do not post their exercise themselves, the results of the linear regression may reflect this more than the actual influence users have on their connections.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b945e157-2ddc-4340-9888-80f507371ed7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
